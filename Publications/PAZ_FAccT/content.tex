\section{Introduction}\label{sec:introduction}

% XAI is all the craze!
Explaining one's behavior to another is a critical element in human social interaction. 
A person depends on explanations to improve their understanding, ultimately building a stable mental model as basis for prediction and control~\citep{heider_psychology_1958}. 
The need to effectively explain not just human action, but also the behavior of automated systems and their underlying \gls{ML} models, has received increasing attention in recent years. This development gave rise to the advent of \gls{XAI} as a novel research field. Consequently, 
% Supposed interpretable models are there, exp CFs, but user exaluations are often neglected.
Consequently, the \gls{XAI} community has seen a veritable surge of technical accounts on how to realize explainability for \gls{ML}~\citep{guidotti_survey_2019}. 

Motivated by a seminal review by~\citep{miller_explanation_2019} advocating a user-centered focus on explainability, \glspl{CFE} gained particular prominence as a supposedly useful, human-accessible solution~\citep{keane_if_2021}.
\glspl{CFE} provide \textit{what-if} feedback to the user, \ie, information on what changes in the input elicit a change of an automated decision (\ie, ``if you had worn a mask, you would not have gotten ill''). 
However, the emerging body of work on \glspl{CFE}, and explainability of \gls{ML} models more generally, shows an alarming tendency to take the quality of the suggested explanation modes at face value \citep{doshi-velez_towards_2017, offert_i_2017}.
In a recent review of over 100 counterfactual XAI studies,~\citeauthor{keane_if_2021} criticize that only a third of these studies concern themselves with user-based evaluations, often with great limitations concerning statistical power and reproducibility~\citep{keane_if_2021}.

% A profound research gap: comparing different approaches:
The lack of user-based evaluations affects not only assessments \glspl{CFE} per se, but more specifically also the evaluation of different conceptualizations for this kind of explanations.
The prevailing approach in the current literature is to compare different \gls{CFE} approaches exclusively in terms of their robustness and theoretical fairness~\citep{artelt_evaluating_2021}, passing over the role of the user as eventual target.  
Thus, in-depth evaluations of user experiences, elucidating the usability of \gls{CFE} variants, are yet to be done.

% Our contribution! 
%% TO DO: DEFINE CONTRIBUTION
The current work marks a step towards closing this fundamental research gap, focusing on the concept of plausibility.
While technical descriptions of plausible \glspl{CFE} approaches exist~\citep{smyth_few_2021,schleich_geco_2021,artelt_efficient_2022}, no user study to date has directly investigated potential benefits of enforcing an additional plausibility constraint on generating \glspl{CFE}.
Therefore, we perform a well-powered user study analyzing the performance of novice users when receiving conventional \glspl{CFE} exclusively defined via their proximity to the decision boundary, compared to algorithmically plausible \glspl{CFE} as feedback in an iterative learning design \citep{artelt_counterfactual_2020, artelt_efficient_2022}.

% The structure of this paper
The remainder of this paper is structured as follows: 
We will first briefly outline \glspl{CFE} as a psychologically grounded mode of explanation, with special regard to the impact of plausibility in terms of counterfactual thought (Section \ref{subsec:psychoCFs}).
Section \ref{sec:EffCompCFs} gives a more technical account of conventional---henceforth referred to as close---and plausible model agnostic \gls{CFE} approaches.
After delineating guiding questions and hypotheses for our research (Section \ref{sec:hypotheses}), we give an in-depth account of our suggested experimental design, as well as the experimental procedure of our user study (Section \ref{sec:experimental-design}). 
Results are depicted in Section \ref{sec:results}, before closing with an in-depth discussion of insights drawn from this study, including limitations and avenues for future work in Section \ref{sec:discussion}.

\section{Counterfactual explanations as a psychologically grounded solution for XAI}\label{subsec:psychoCFs}

% we still don't know what a good explanation is
A major challenge for \gls{XAI} is the lack of a common, straight-forward and universally applicable definition of what constitutes a good explanation.
To complicate matters, the effectiveness of an approach may depend on the reason for explaining \citep{adadi_peeking_2018}, as well as pre-existing knowledge and experiences of users at the receiving end \citep{van_der_waa_evaluating_2021}.

% insights from psychology can help!
In search of truly human-usable explanation modes, the \gls{XAI} community recognized the need to bridge the gap between psychology and computer science in order to draw inspiration from how humans explain in their daily social interactions~\citep{miller_explanation_2019}.
A central insight from classical psychological literature is that human explanations are typically contrastive: They emphasize (explicitly or implicitly) why a specific outcome occurred instead of another~\citep{miller_explanation_2019, lipton_contrastive_1990, lombrozo_explanation_2012, hilton_knowledge-based_1986}.

% from contrastive explanations to counterfactual thinking / counterfactural reasoning / CF explanations
This contrastive nature relates to the more general human tendency to reflect upon past events by generating possible alternatives, \ie, counterfactual thinking ~\citep{roese_counterfactual_1997}.
Empirical evidence demonstrates that humans show this \textit{what-if} mentality spontaneously~\citep{goldinger_blaming_2003}, and increasingly when facing negative outcomes or unexpected results~\citep{sanna_antecedents_1996}.
In their \textit{Functional Theory of Counterfactual Thinking},~\citeauthor{roese_functional_2017} suggest a crucial role of counterfactual thought to guide to formation of future intentions, thus regulating subsequent behavior~\citep{roese_functional_2017,epstude_functional_2008}.
This evidence is the root for the common supposition in \gls{XAI} that explanations formulated as counterfactuals are naturally intuitive, easy to understand, and helpful for users, often discounting the need for user evaluations~\citep{stepin_paving_2019, dandl_multi-objective_2020, guidotti_local_2018, artelt_counterfactual_2020, artelt_efficient_2022}.

\subsection{Humans generate plausible CFEs}\label{subsubsec:psychoCFs_plausible}

Decades of philosophical and psychological research has concerned itself with the question of how humans generate counterfactuals. 
% how do humans generate CFEs? --> possible worlds account by Lewis
Lewis' seminal work on the topic builds on a theory of possible worlds, postulating that counterfactual statements trigger a comparison between the actual circumstances and a conceivable world in which the counterfactual statement occurred~\citep{harper_counterfactuals_1973}.
% how do humans generate CFEs? --> mental models view, Byrne
Embedding this view into a cognitive framework of counterfactual thought, the Mental Models Theory emphasizes the human ability to entertain two parallel representations of reality: The factual conditional, corresponding to the true state of the world, and the concurrent non-factual possibility, temporarily assumed to be true~\citep{byrne_mental_2002,byrne_precis_2007,johnson-laird_conditionals_2002,walsh_mental_2005}.
% Neuroscience evidence:
Insights from neuroimaging support this notion, demonstrating that counterfactual thinking extends mere hypothetical deliberation by recruiting additional representational processes in the brain~\citep{kulakova_processing_2013}.

% Authors often note that humans prefer 
When humans generate counterfactuals, they show remarkable regularities in terms of which aspects of the past they reconstruct. 
Humans tend to modify events that are recent~\citep{miller_temporal_1990, byrne_temporality_2000}, exceptional, while also regarding the optimal counterfactual outcome~\citep{kahneman_simulation_1982, dixon_if_2011}, and 
controllable events when undoing of fictitious outcomes~\citep{girotto_event_1991}.
Further, authors like to note that humans produce plausible rather than implausible counterfactuals~\citep{byrne_counterfactual_2016, de_brigard_coming_2013}.

% What is plausible from a psycho point of view? Hard to say
Despite being a commonly-used notion in psychology, however, plausibility is difficult to define precisely.
Variable interpretations of what constitutes a plausible counterfactual exist, referring to different partially overlapping concepts.

\citeauthor{kahneman_simulation_1982} refer to hypothetical events as plausible if they are easy to imagine~\citep{kahneman_simulation_1982}. 
%%%%%%% plausibility as closeness / similarity / comparative similarity:
In his seminal work,~\citeauthor{harper_counterfactuals_1973} supposes that plausible counterfactuals come from worlds that are minimally different from reality~\citep{harper_counterfactuals_1973}. 
Building up on this idea of comparative similarity, empirical research shows perceived plausibility of a counterfactual event to be proportional to the perceived similarity between said counterfactual and the factual state~\citep{stanley_counterfactual_2017, de_brigard_perceived_2021}.

%%%%%%% plausibility as probability:
In addition to this similarity-based definition, plausibility is often used synonymously with concepts of likeliness or probability~\citep{pezdek_is_2006, de_brigard_remembering_2013}. 
\citeauthor{de_brigard_remembering_2013} demonstrate that manipulations of counterfactual plausibility in terms of their likeliness yields changes their neural representation~\citep{de_brigard_remembering_2013}.
Their findings may indicate greater affective evaluation for counterfactuals that carry greater subjective likelihood, and thus, plausibility.
In their Plausibility Analysis Model,~\citeauthor{connell_model_2006} expand on the idea of plausibility as probability and highlight the pivotal role of pre-existing domain knowledge, postulating that a scenario may only be plausible if it fits well to prior knowledge~\citep{connell_model_2006}.

% Concluding, and bring it together with automatic CFEs
Thus, while it is difficult to pinpoint exactly what makes a counterfactuals psychologically plausible, we may recognize pivotal roles of concepts like comparative similarity and probability.
Following the user-centered focus on explainability proposed by~\citeauthor{miller_explanation_2019}~\citep{miller_explanation_2019}, incorporating these concepts would be an important step towards automatic generation of plausible, and thus more human-friendly and usable, \gls{CFE}.

\section{Computation of CFs and plausible CFs}\label{sec:EffCompCFs}

\textcolor{ACMDarkBlue}{}
%In the computer science world, generating a CFs translates into showing what input pertubations alter a model's prediction. Additionally, some authors use the constraint that these pertubations need ot be minimal (REF REF REF).
%Changes need to be actionable to make a good explanation (REF), i.e. to also foster trust in the system (REF).
%Counterfactuals are a good means to provide these explanations, as they do not require models themselves to be interpretable, and do not disclose details about an algorithms's internal logic, or other data samples in the training set~\citep{Wachter2017}.
%Given the surge of CF methods lately, there are a lot of ways CFs are computed (REFs REFs REFs)...
%Importantly: there is some variation in terms of how CFs are defined: Sahil Verma, John Dickerson, and Keegan Hines. 2020. Counterfactual Explanations for Machine Learning: A Review. (2020).
%In the current paper, we rely on the work of Artelt et al, and they do it like this...

\section{Do users profit from algorithmic plausibility in an abstract domain?}\label{sec:hypotheses}

% Here: Make a connection why it would be sensible to assume so.
% Plausibility is an important feauture for human-generated CFEs, so why not also for machine learning ones?

\textcolor{ACMDarkBlue}{
The main research question of the current work is: Can plausible \glspl{CFE} help users?
}

\textcolor{ACMDarkBlue}{
We posit: yes! Maybe being repeateldy exposed to exemplars that are more representative of the real distribution enables humans to build a more appropriate mental model more quickly. While no background knowledge exsists at the start, the real life situation may be really clear for people after seeing these examples (more than CFs that are just minimally different, but not representative of the real data distribution)
}

\textcolor{ACMDarkBlue}{
Further Guiding questions:
Do plausible \gls{CFE} facilitate learning?
Do plausible \glspl{CFE} increase user's subjective understanding?
Given the robustness advantage of plausible \glspl{CFE} over closest \glspl{CFE}, and given that we know that plausibility helps users understand (TO DO!), we formulated the following 4 hypotheses.
}
First, we expect plausible \glspl{CFE} to be more helpful to users tasked to discover unknown relationships in data than closest ones, both in terms of objective and subjective understandability (hypothesis 1). 
Specifically, we expect participants in the plausible condition a) to produce larger packs over time, b) to become more automatic and thus quicker in choosing their plants, and c) to clearly state which plants were crucial for their pack to prosper. % (survey items 1 and 2).

Second, we expect to see difference in terms of subjective understanding between both groups (hypothesis 2).
We predict that users will differ in how far they find the explanations useful, and in how far they can make use of it, with an advantage of plausible CFEs. %(survey items 5, 6).
We also posit that users imagine plausible CFEs to be more helpful for others users. %(survey item 9).

An important feature of the current design is the high comparability of conditions, as structure and presentation mode of \glspl{CFE} is kept constant for both groups.
Thus, we do not expect users in different conditions to differ in their understanding of the explanations per se, their need for support to understand, and their evaluation of timing and efficacy of \gls{CFE} presentation (hypothesis 3).
% (questionnaire items 3, 4 + 10). So this is also control to make sure groups don’t differ in a weird way.

Finally, we do not formulate a prediction whether groups with differ in uncovering inconsistencies in the explanations presented. This will be investigated in an exploratory analysis.% (maybe that happens in case of “closest” CFEs when we’re in the areas of “no training data“?) (questionnaire item 8). 

\section{Experimental Design}\label{sec:experimental-design}
\textcolor{ACMDarkBlue}{
To test hypothesis1,hypothesis2,hypothesis3, ... from \ref{sec:hypotheses}, we... used the AlienZoo paradigm (REF to arxiv when it's there). 
}

\subsection{The plausible Alien Zoo paradigm}

\textcolor{ACMDarkBlue}{
In the current study, be relied on the novel Alien Zoo paradigm (REF to arxiv paper).
} \textcolor{ACMDarkBlue}{
In this scenario, participants are asked to imagine themselves as zoo keepers in a zoo for aliens. Several alien plants may be chosen to feed to the aliens, but it is not clear what plants make up a nutritious diet for the aliens. Thus, participants are tasked to find how to best feed the aliens.
Participants go through several feeding cycles, choosing a combination of plants for feeding. After each cycle, the pack of aliens either decreases (given a bad combination of plants) or increases (given a good combination). 
In regular intervals, participants receive a summary of their past choices, together with feedback on what choice would have led to a better result (\ie, a counterfactual explanation). 
(For details on the procedure, see \ref{subsec:experimental-procedure}).
} \textcolor{ACMDarkBlue}{
Advantages of this approach is control for individual domain knowledge by relying on an abstract domain, evaluating the objective as well as subjective usability of automatically generated explanations directly (REF to arxiv paper).
} \textcolor{ACMDarkBlue}{
Thus, this current use case corresponds to a human grounded evaluation following the taxonomy by~\citeauthor{doshi-velez_towards_2017}, assessing perforemance of real users in an abstract task setting.
} \textcolor{ACMDarkBlue}{
Unlike most real-life applications, systems investigated in user studies typically focus on applications for expert users trying to gain expert knowledge of a domain. 
In contrast, we investigate whether providing \glspl{CFE} to novice users improves their understanding of relationships in a yet unknown dataset. 
Thus, our setting falls under the explaining to discover reasons for explainability defined by~\citeauthor{adadi_peeking_2018}~\citep{adadi_peeking_2018}.
Specifically, we focus on the question whether \glspl{CFE} computed according to algorithmic plausibility prove to be more useful for novice users in this setting than providing closest \glspl{CFE}, independent of domain.
Relying on novice users in an abstract context, we mitigate any difference in domain knowledge and possible misconceptions about the task setting~\citep{van_der_waa_evaluating_2021}.
}

\subsection{Post-Game survey}
\textcolor{ACMDarkBlue}{
The final post-game questionnaire is used to collect self-report information from participants. Participants are asked several Likert-scale questions, inspired by the System Usability Scale~\citep{holzinger_measuring_2020}, a Werkzeug designed to measure the quality of explanations elicited by an explainable ML system.
Its purpose is to obtain an understanding of how users feel about using our system, including whether they trusted and understood the system and explanations.
}

\subsection{Constructs, expected relations and measurements}

\textcolor{ACMDarkBlue}{
\textbf{Maybe here: set up and add a causal diagram of our study as in van der Waa (2021)?}
} \textcolor{ACMDarkBlue}{
We measure understanding and usability of explanations in terms of two objective behavioral variables and several subjective self-reports. 
} \textcolor{ACMDarkBlue}{
In terms of behavior, we assess the development of pack size in the Alien Zoo game over trials. 
As a measure of task performance, this value indicates the extend of user's understanding of relevant in irrelevant features in the underlying dataset, assuming that a solid understanding leads to better feeding choices. 
} \textcolor{ACMDarkBlue}{
Second, we measure time needed to reach a feeding decision over trials. As we assume participants to become more automatic in making their plant choice, we expect this practice effect to be reflected in terms of decreasing time required to reach a feeding decision~\citep{logan_shapes_1992}.
} \textcolor{ACMDarkBlue}{
We acquired self-reported measurements using the post-game survey, testing different aspects of participant’s system understanding. 
} \textcolor{ACMDarkBlue}{
The first two survey items explicitly ask users to identify plants they think are relevant and irrelevant for task success. 
Replies from these items allow us to measure to shich extend users in different groups formed explicit knowledge of the underlying data structure. 
} \textcolor{ACMDarkBlue}{
Further, users indicate how far they find the explanations useful, in how far they can make use of it, and in how far they imagine the presented \glspl{CFE} to be helpful for others users, too. These items assess users subjective understanding.
} \textcolor{ACMDarkBlue}{
Finally, three self-reported measurements check for potential confounds. 
These are items that ask users to indicate their understanding of the explanations per se, whether they feel the need for support for understanding, and their evaluation of timing and efficacy of \gls{CFE} presentation.
Given that structure and presentation mode of \glspl{CFE} is kept constant for both groups, differences would uncover unexpected variation in terms of answer style across groups, which would be a confounding variable. 
}

\subsection{ML model and implementation}
\textcolor{ACMDarkBlue}{
The web interface was developed using [REF to phaser, javascript, etc.].
} \textcolor{ACMDarkBlue}{
The behavior of the game is determined by predictions of underlying ML models. 
} \textcolor{ACMDarkBlue}{
While technically, any ML algorithm (that is CEML cimpatible) could be used, we chose to use ...
% TO DO: explain 
}

\subsection{Participants}

\textcolor{ACMDarkBlue}{
We developed a game-like experimental design based on a web-based interface, to facilitate access for users from all over the world and diverse backgrounds, enabling large-scale and easy participant recruitment.
}

The study ran in early November 2021 on \gls{AMT}.
After performing three pilots with 10 users each to refine the experimental design, we recruited a total of 100 participants for the final assessment. % Batch info neccesary? % report that users were excluded if the done it before?
A first data quality check revealed corrupted data for four participants due to logging issues. Thus, we acquired four additional data sets. 
All participants gave informed electronic consent by providing clickwrap agreement prior to participation.
All participants received a reward of US\$ 4 for participation. 
The ten best performing users received an additional bonus of US\$ 2. 
Game instructions informed participants about the possibility of a bonus to motivate compliance with the experimental task \citep{bansal_updates_2019}.
The study was approved by the Ethics Committee of the University Bielefeld, Germany.

\subsection{Experimental Procedure}\label{subsec:experimental-procedure}

\textbf{Here: insert image(s) of game UI and study flow.}

After accepting the task on \gls{AMT}, participants are forwarded to our web server hosting the alien zoo game.
They first encounter a page informing them about purpose, procedure and expected duration of the study, their right to withdraw, confidentiality and contact details of the primary investigator.
Users may decline to participate by closing this window.
Otherwise, they indicate their agreement via button press, opening a new page.
Unbeknownst to the user, they are randomly assigned to either the ``closest'' or the ``plausible'' condition when they indicate agreement.

The succeeding page provides detailed instructions to the game. 
Specifically, it shows images of the aliens, as well as the selection of plants they may choose to feed from.
Written instructions detail that it is possible to choose up to six leaves per plant in whatever combination seems desirable, and that choosing healthy or unhealthy combinations lead to increases or decreases in pack size, respectively.
Further instructions emphasize the user's task to maximize the number of shubs, with the best players qualifying for a monetary bonus.
Participants are also informed that they will receive feedback on what choice would have led to a better result after two rounds of feeding.
Users may indicate that they are ready to begin the game by clicking a ``Start'' button at the end of the page.
To prevent participants from skipping the instructions, this button appears with a delay of 20s.

Upon hitting ``Start'', participants encounter a padlock scene where they can make their feeding choice.
The right side of the screen displays leaves from all plant types next to upward/downward arrow buttons. 
In the first feeding round, the top of the page shows written information that clicking on the upward arrows increases the number of leaves per plant, while clicking the downward arrows has the reverse effect.
In each succeeding feeding round, the top of the page shows the current number of Shubs, the number of Shubs in the previous round, and the choice made in the previous round.
The page additionally shows a padlock with the current number of animated Shubs.
After making their choice, participants continue by clicking a button stating ``Feeding time!'' in the bottom right corner of the screen.

Upon committing their choice, a progress scene displaying the current choice of plants and three animated Shubs is shown. 
Meanwhile, the underlying \gls{ML} model uses the user input to generate the new growth rate and pack size, together with either a closest or a plausible counterfactual.
After 3s, the padlock scene appears again to show the results of their last choice. 
Following odd trials, the user may make a new selection. 
After even trials, a single ``Get feedback!'' button replaces the choice panel on the right-hand side of the screen.
Hitting the feedback button forwards a user to an overview scene displaying the feeding choices in the last two runs, the resulting changes in number of shubs and the counterfactuals that indicate what choices would have led to better results. 
When users made a choice that led to maximal increase in pack size such that no counterfactual could be computed, they are told that they were close to an optimal solution in that round. 
Users may move on to the next round by hitting a ``Continue!'' button appearing after 10s on the right-hand side of the screen. This delay forces users to spend some time with the information to study it. Upon continuing, users make their new choice in a new padlock scene.

The study runs over 12 feeding rounds (trials) with feedback interspersed after each second trial. 
To ensure attentiveness of users during the game, we included two additional attention trials.
After feeding rounds 3 and 7, users face a new page requesting to type in the current number of shubs in their respective packs.
Immediate feedback informs participants whether their entry was correct or not, and reminds users to pay close attention to all aspects of the game at any given time.
Subsequently, the next progress scene appears and the game continues. 

The game part of the study is complete after 12 trials.
The experimental procedure concludes with a survey assessing user's explicit knowledge on what plants were and were not relevant for improvement (items 1 and 2), as well as an adapted version of the System Causability Scale \cite{holzinger_measuring_2020} evaluating the subjective quality of explanations.
The study closes with two items assessing demographic information on gender and age.
The final page thanks users for their participation and provides a unique code to insert in \gls{AMT} to prove that they completed the study and qualify for payment. 
Further, participants may choose to visit a debriefing page with full information on study objectives and goals. 

On average, participants needed 13m:43s ($\pm$ 00m:23s SEM) from accepting the HIT on AMT to inserting their unique payment code.
%this code was stored in crypted format and deleted as soon as payment was done - this ensured anonymity

\subsubsection{Statistical Analysis, Sample Size Calculation and Data Quality Measures}

We perform all statistical analyses using R–4.1.1 \citep{r_core_team_r_2021}, using \gls{CFE} variant (closest or plausible) as independent variable.
Changes in performance over 12 trials as a measure of learning rate per group (lme4 v.4\textunderscore 1.1-27.1) \citep{bates_fitting_2015}.
In the model testing for differences in terms of user performance, the dependent variable is number of Shubs generated. 
In the assessment of user's reaction time, we use time needed to reach a feeding decision in each trial as dependent variable.
The final models include the fixed effects of group, trial number and their interaction. The random-effect structure includes a by-subjects random intercept. 
Advantages of using this approach include that these models account for correlations of data drawn from the same participant and missing data \citep{detry_analyzing_2016,muth_alternative_2016}.
The analysis of variance function of the stats package in base R serves to compare model fits.
$\eta_{\text{p}}^{2}$ values denote effect sizes (effectsize v.0.5) \citep{ben-shachar_effectsize_2020}.
Computation of pairwise estimated marginal means follow up significant main effects or interactions, with respective effect sizes reported in terms of Cohen’s \textit{d}.
All post-hoc analyses reported are bonerroni corrected to account for multiple comparisons.

We evaluate data gathered from the post-game survey depending on question type.
For the first two items assessing user's explicit knowledge of plant relevance, we test data for normality of distributions using the Shapiro-Wilk test, followed up by the non-parametric Wilcoxon-Mann-Whitney \textit{U} test in case of non-normality, and the Welch two-sample t-test otherwise. 
We follow the same approach to compare age and gender distributions.
To analyse group differences of ordinal data from the likert-style items, we rely on the non-parametric Wilcoxon-Mann-Whitney \textit{U} test.
We report effect sizes for all survey data comparisons as \textit{r}.

We performed an a priori sample size estimation based on data obtained from the third pilot. 
To do so, we set up two linear mixed effects models with shub number and reaction time as described above. 
For each of these models, we run a simulation-based power analyses for samples of 20--100 participants with fixed effects of group and trial number over 1000 simulations (mixedpower v.0.1.0) \citep{kumle_estimating_2021}. 
Inspecting the results, we choose to acquire data from 100 participants to reach a power $>$ 80\% for at least one combination of trial number and group.

As a web-based study, we run the risk that some participants attempt to game the system to collect the reward without providing proper answers. 
Thus, we implemented a number of data quality checks that were planned a priori.
We identify speeders based on the time needed to reach a feeding decision, flagging users that spent less than 2s in the padlock scene in 4 or more trials.
We flag participants that fail to respond with the correct number of shubs in both attention trials during the game.
Furthermore, we included a catch item (item 7) in the survey, asking users to tick the ``I prefer not to answer.'' option, flagging users that did not pay attention to the survey items.
Finally, we identify straight-lining participants who keep choosing the same plant combination despite not improving in at least two blocks, or answer with only positive or negative valence in the survey.
To uphold a high threshold for data quality, we follow a conservative approach of excluding participants that were flagged for at least one of these reasons.

\section{Results}\label{sec:results}

From 100 participants recruited via \gls{AMT}, we exclude data from participants who qualified as speeders (\textit{n} = 2), failed both attention trials during the game (\textit{n} = 5), gave an incorrect response for the catch item in the survey (\textit{n} = 3), or straight-lined during the game (\textit{n} = 4) or in the survey (\textit{n} = 12), leaving data from 74 participants for final analysis (Table~\ref{tab:participants}).

\begin{table}
  \caption{Demographic information of participants.}
  \label{tab:participants}
\begin{tabular}{llllllllll} 
\toprule
    & \multicolumn{4}{l}{Before quality assurance measures (\textit{N} = 100)} && \multicolumn{4}{l}{After quality assurance measures (\textit{N} = 74)} \\
\cline{2-5}\cline{7-10}
    & Closest & Plausible & \textit{U} value$^a$ & \textit{p} value && Closest & Plausible & \textit{U} value$^a$ & \textit{p} value\\ 
\hline
\textit{N}   &  50 & 50 & .. & .. && 40 & 34 & .. & .. \\
Gender$^b$ & 17f/33m & 22f/26m/1nb/1na & 1108 & .339 && 13f/27m & 18f/15m/1nb & 554.4 & .116 \\
Age (\textit{Mdn})$^c$ & 25--34y & 25--34y & 1234 & .950 && 25--34y & 35--44y & 712.5 & .718 \\
\bottomrule
\multicolumn{9}{l}{$^a$ non-parametric Wilcoxon-Mann-Whitney \textit{U} test}\\
\multicolumn{9}{l}{$^b$ f = female, m = male, nb = non-binary / gender non-conforming, na = no gender information disclosed}\\
\multicolumn{9}{l}{$^c$ \textit{Mdn} = median age band (options: 18-24y, 25-34y, 25-34y, 35-44y, 45-54y, 55-64y, 65y and over)}
\end{tabular}
\end{table}

\subsection{Do plausible CFEs facilitate learning?}

Hypothesis 1 postulates that users in the plausible condition outperform users in the closest condition.
To statistically assess this hypothesis, we compare data from participants in both groups in terms of pack size produced over time, time needed to reach a feeding decision, and matches between ground truth and indicated plants.
Figure \ref{fig:hyp1}a shows the development of average pack size as well as average time spent to reach a feeding decision per group. 
Strikingly, the data suggests that participants in the closest, not the plausible, condition performed better. 
This effect is confirmed by the significant interaction of factors \textit{trial number} and \textit{group} (\textit{F}(11,792) = 2.1193, \textit{p} = 0.017, $\eta_{\text{p}}^{2}$ = 0.0286) in the corresponding linear mixed effects model. The follow-up analysis reveals significant differences between groups in trial 11 (\textit{t}(472) = 4.040, \textit{p} = .0.012, \textit{d} = 0.6929) and trial 12 (\textit{t}(472) = 2.530, \textit{p} \textless .001, \textit{d} = 1.1010).
Additionally, there is a highly significant main effect of trial number (\textit{F}(11,792) = 7.5851, \textit{p} \textless .001, $\eta_{\text{p}}^{2}$ = 0.0953), but no significant main effect of group (\textit{F}(11,72) = 2.5857, \textit{p} = .112, $\eta_{\text{p}}^{2}$ = 0.0347).

Participants in both groups showed a marked decrease in time needed to reach a feeding decision over the curse of the study, already apparent after the very first trial (Figure \ref{fig:hyp1}b).
The significant main effect of factor \textit{trial number} (\textit{F}(11,792) = 14.8177, \textit{p} \textless .001, $\eta_{\text{p}}^{2}$ = 0.1707) confirms this observation.
Corresponding post-hoc analyses show significant differences between trial 1 and all other trials (all \textit{t}(792) \textgreater 5.90, \textit{p} \textless .001, \textit{d} \textgreater 1.2000), between trial 3 and 4 (\textit{t}(792) = 3.765, \textit{p} = 0.012, \textit{d} = 0.6210), and between trials 4 and 5 (\textit{t}(792) = 3.395, \textit{p} = 0.048, \textit{d} = 0.5600).
Neither the main effect of factor \textit{group} (\textit{F}(11,72) = 0.2347, \textit{p} = .630, $\eta_{\text{p}}^{2}$ = 0.0032), nor the interaction between factors \textit{trial number} and \textit{group} (\textit{F}(11,792) = 0.8966, \textit{p} = .543, $\eta_{\text{p}}^{2}$ = 0.0123) reach significance.

In terms of mean number of matches between user judgements of plant relevance for task success and the ground truth, users in both groups performed comparably both for relevant 
(closest: mean number of matches = 2.8500 $\pm$ 0.1979 \textit{SE}; plausible: mean number of matches = 3.2060 $\pm$ 0.1780 \textit{SE}; \textit{U} = 781, \textit{p} = 0.255, \textit{r} = 0.0540)
and irrelevant plants (closest: mean number of matches = 3.125 $\pm$ 0.1568 \textit{SE}), plausible: mean number of matches = 3.1765 $\pm$ 0.2172 \textit{SE}; \textit{U} = 721.5, \textit{p} = .643, \textit{r} = 0.0539).

Thus, we cannot verify our hypothesis that plausible \glspl{CFE} facilitate learning. On the contrary, the development of pack size between the groups points to the opposite effect of closest \glspl{CFE} being more beneficial for users than plausible ones.

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{./media/H1_p_ShubsPerTrial_RTPertrial_PAZ_FINAL.pdf}
   \Description[TO DO: insert short description]{TO DO: insert description}
   \caption{Development of \textbf{a} mean pack size per group by trial, \textbf{b} mean time needed to reach a feeding decision per group by trial, and \textbf{c} mean number of matches between user judgements and ground truth for survey items assessing relevant plants and irrelevant plants, respecively. Shaded areas in \textbf{a} and \textbf{b}, and error bars in \textbf{c} denote the standard error of the mean. Asterisks denote statistical significance at \textit{p} < 0.05 (*) and \textit{p} < .001 (***), respectively.}
   \label{fig:hyp1}
 \end{figure}

\subsection{Do plausible CFEs increase user's subjective understanding?}
To assess hypothesis 2, we analyze participant judgements on relevant survey items.
Visual assessment suggests that there is very little variation in terms of user responses between groups (Figure \ref{fig:survey}a), confirmed by our statistical assessment. 
Groups do not statistically differ when judging whether presented \gls{CFE} feedback was helpful to increase pack size (closest condition: \textit{M} = 3.7000 $\pm$ 1.2850 \textit{SE}; plausible condition: \textit{M} = 3.6364 $\pm$ 0.2416 \textit{SE}; \textit{U} = 656, \textit{p} = .968, \textit{r} = 0.0047).
Likewise, we do not detect significant group differences in terms of subjective usability (closest condition: \textit{M} = 3.7750 $\pm$ 0.2163 \textit{SE}; plausible condition: \textit{M} = 3.6061 $\pm$ 0.2300 \textit{SE}; \textit{U} = 603, \textit{p} = .513, \textit{r} = 0.0765).
In addition, there is no significant difference between groups for estimated usefulness of explanations for others (closest condition: \textit{M} = 3.7500 $\pm$ 0.2080 \textit{SE}; plausible condition: \textit{M} = 3.6471 $\pm$ 0.2063 \textit{SE}; \textit{U} = 637, \textit{p} = .631, \textit{r} = 0.0558).

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{./media/H2_H3_expl_survey_T_PAZ_FINAL.pdf}
   \Description[TO DO: insert short description]{TO DO: insert description}
   \caption{Overview of user judgements in post-game survey per group, adapted from ~\citep{holzinger_measuring_2020}. \textbf{a} depicts user replies in survey items relevant for hypothesis 2, \textbf{b} depicts user replies in survey items relevant for hypothesis 3, and \textbf{c} depcits replies relevant for our last exploratory analysis. Distributions did not differ significantly between groups for any of the items (all \textit{p} \textgreater 0.05).}
   \label{fig:survey}
 \end{figure}

\subsection{Does mode of presentation have an impact?}
As postulated in hypothesis 3, we do not observe group differences between conditions in terms of understanding the explanations per se (Figure \ref{fig:survey}b). 
A considerable proportion of both groups responded positively about understanding the feedback, not differing significantly in their responses (closest condition: \textit{M} = 3.9750 $\pm$ 0.1843 \textit{SE}; plausible condition: \textit{M} = 4.1176 $\pm$ 0.1622 \textit{SE}; \textit{U} = 773.5, \textit{p} = .200 \textit{r} = 0.1489).
In terms of needing support for understanding, both groups reply with a similar response pattern (closest condition: \textit{M} = 3.2000 $\pm$ 0.2298 \textit{SE}; plausible condition: \textit{M} = 3.1471 $\pm$ 0.2573 \textit{SE}; \textit{U} = 667, \textit{p} = .890 \textit{r} = 0.0161).
Similarly, user judgements on timing and efficacy of  CFE presentation are consistently high across groups (closest condition: \textit{M} = 4.1000$\pm$0.1747 \textit{SE}; plausible condition: \textit{M} = 4.1471$\pm$0.1696 \textit{SE}; \textit{U} = 680.5, \textit{p} = 1 \textit{r} = 0.0000).

\subsection{Exploratory Analysis}
Our explanatory analysis revealed that more than half of all users in both groups did not detect inconsistencies in the \glspl{CFE} provided (closest condition: \textit{M} = 2.6750 $\pm$ 0.1656 \textit{SE}; plausible condition: \textit{M} = 2.8529 $\pm$ 0.2074 \textit{SE}; \textit{U} = 743, \textit{p} = 0.480 \textit{r} = 0.0820).

\section{Discussion}\label{sec:discussion}

% re structure along the main questions:
%\subsection{Do plausible \gls{CFE} facilitate learning?}

\textcolor{ACMDarkBlue}{
% for our case: %%%%%% MOVE TO DISCUSSION: %%%%%%%%%
Why is closer better? since we are in an abstract domain - no matter which definition of psychological plausibility we take (likeliness, ease of imagining it) - it deviates seems to be pointless given the abstract domain; all CFEs should be equally likely, all should be equally likely to imagine (MAYBE NOT, THOUGH: plausible CFEs induce larger jumps, larger difference, maybe increasing the mental load on users, making it more difficult to apply them) -- in fact, according to the mental models view, the closest CFEs may be more psychologically plausible than the algorithmically plausible version.  
when people naturally generate counterfactual thoughts, they tend to contrast a template mental representation of what is thought to be true or normal against another mental representation that is thought to be false but deviates minimally from the template. 
THIS MIGHT BE AN "AFFECTIVE ACCOUNT" as interpretation: in fact, semantic closeness is related to human plausibility -- this is the debate about "just making it" - "just missing the better option" -- this has shown to lead to more negative emotions--thus leading to greater drive to improve? CHECK LIT AGAIN!
} \textcolor{ACMDarkBlue}{
Here's a thing:
It is notable that a considerable proportion of both groups responded positively about understanding the feedback, supporting the general notion that \glspl{CFE} are very human friendly.
The highest agreement is inpresentation of feedback (timely and efficiently), which is a win for our design!
} \textcolor{ACMDarkBlue}{
What we can also show: the framework of the alien Zoo is suitable to test the usability of a particular CF method. We could also use it to test different CF methods agains each other! Such comparative user studies are sparse: Akula, et al., 2020; Förster et al., 2020a, 2020b (referenced in Keane, 2021)
} \textcolor{ACMDarkBlue}{
Lim 2019 writes: ''Our results may suggest the ineffectiveness of How To and What If explanations, but these intelligibility types may be more useful for other types of tasks, particularly those relating to figuring out how to execute certain system functionality, rather than interpreting or evaluating.'' - are we better? Why?
} \textcolor{ACMDarkBlue}{
What could lead to problems? From Lage, 2019: ''Kulesza et al. (2013) performed a qualitative study in which they varied the soundness and the completeness of an explanation of a recommendation system. They found completeness was important for participants to build accurate mental models of the system.'' -- Are CFs really complete? Is it guaranteed that user's get the complete picture given a subset of explanations highly dependent on their own input?
} \textcolor{ACMDarkBlue}{
Lage et al., 2019: CF evaluation is harder than mere simulation or verification tasks (i.e., users need significantly longer and were judged as being significantly harder to do than simple simulations)
} \textcolor{ACMDarkBlue}{
Check out: improved task performance are contingent on both system understanding and appropriate trust (CHEF REF: R.R. Hoffman, S.T. Mueller, G. Klein, J. Litman, Metrics for explainable AI: challenges and prospects, arXiv preprint arXiv:1812.04608.)
} \textcolor{ACMDarkBlue}{
in van der Waa 2021: explanations did not improve task performance (choosing the correct dosage of insulin) - why did it (hopefully) work for us?
} \textcolor{ACMDarkBlue}{
Building upon work by Isabel Valera: in our case, plausibility corresponds to algorithmic recourse, as all changes in features are independent (user can change leaf 1, and this will have no long-time effect on leaf 2), and all changes are feasible (i.e., doable for the participant). In real life, this is not always the case (a bank customer might never be able to get younger to get a loan; yearly income also affects savings) - thus, our example is more artificial
} \textcolor{ACMDarkBlue}{
Our findings are a replication of a phenomenon observed by van der Waa, 2021: in a recent study, van der Waa showed that perceived system understandability did not correlate with any objective measure of performance, shedding profound doubt onto whether we should just assume something to be understandable.
}

\subsection{Limitations \& Future Work}\label{subsec:limitations-future-work}
\textcolor{ACMDarkBlue}{
Two critical design decisions need to be considered are the reason for explaining, and who the given explanation targets. Thus, it is important to keep in mind, that ganeralizability of results of user based studies is likely limited to the current reason (to 'explore') as well as the current target audience (novice users). 
Our experimental design caters to one need for explanation only. 
We can only make statements as to how CFs are useful when ML predictions are used to learn something about the system. 
This might prove super helpful for ML tutoring systems. 
However, there are more reasons to explain (debugging, etc., see Adadi again) that the current design cannot cover. Future work and other settings are needed.
} \textcolor{ACMDarkBlue}{
Out design has the advantage to be very generall, addressing the lay user. For CFs that are supposed to assist experts in very specific applications, e.g., in the medical (REF) field or so (others?) need more specific user assessments and experimental designs targeting experts.
} \textcolor{ACMDarkBlue}{
From a psychological standpoint, it is important to note that definitions of CFs slightly diverge. Many computer science definitions consider a CF as valid only if it represents a ''minimal set of features to change'' (Ref?). In Psychology, however, any combination of changes leading to a different outcome can be considered a counterfactual (true, there are better and worse ones, but still...)
} \textcolor{ACMDarkBlue}{
Out study was set up to be engaging - but it is also complex; there is a chance that users could have been overwhelmed with the user interface
} \textcolor{ACMDarkBlue}{
Impact of individiual traits on performance here could not be tested (see Gleaves et al., 2020). Future work!
} \textcolor{ACMDarkBlue}{
Abstract domain - probably not suitable for other specific domain such as CFs for image classification
} \textcolor{ACMDarkBlue}{
Many XAI studies supposedly suffer from confirmation bias - does our do so, too?: Rosenfeld, A. (2021, May). Better Metrics for Evaluating Explainable Artificial Intelligence. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems (pp. 45-50).
} \textcolor{ACMDarkBlue}{
Another cool design worth looking at: chatbot use! Chat-XAI: A New Chatbot to Explain Artificial Intelligence, 2021; \url{Gao, M., Liu, X., Xu, A., \& Akkiraju, R. (2021, September). Chat-XAI: A New Chatbot to Explain Artificial Intelligence. In Proceedings of SAI Intelligent Systems Conference (pp. 125-134). Springer, Cham.}
} \textcolor{ACMDarkBlue}{
Future work:
Our design has tremendous potential to be adapted to answer more questions...
} \textcolor{ACMDarkBlue}{
Lim, 2009: ''Another issue with real systems is that users may not like to receive explanations all the time, but on demand instead, because the former may be too obtrusive. We would like to run a future study to compare if users can still benefit sufficiently from explanations if they get to choose when and how often they can receive explanations, and if explicit effort in asking for explanations can improve learning.'' Adapt this point for our purpose as well!
} \textcolor{ACMDarkBlue}{
Whithin-subject study would be cool!
} \textcolor{ACMDarkBlue}{
Look at various age groups (including kids!) and explore the effect other 
}
\textcolor{ACMDarkBlue}{
Did we use an unsuitable design / setting?
One thing in our case: "plausibility is usually confounded with degree of background knowledge." (PEZDEK, 2006,  Memory \& Cognition) - none of our people had background knowledge (sold as an advatage so far)  
}
\textcolor{ACMDarkBlue}{
Future work: plausibility may be dangerous: one may assume that something is true, just because it is plausible (i.e., the plausibility fallacy)
}

\subsection{Conclusion}\label{subsec:conclusion}
\textcolor{ACMDarkBlue}{
In this work, we present a large controlled study of...
We developed a game-like experimental design based on a web-based interface...
The code of this is available, making it easy to adapt to other approaches to compute CFs and other underlying ML models. We welcome its use by other research groups and practitioners, to further advance the currently limited insights into the field.
} \textcolor{ACMDarkBlue}{
Our findings suggest that providing (plausible) counterfactuals explanations to novice users...
}