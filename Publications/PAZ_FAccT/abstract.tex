\Glspl{CFE} -- highlighting what changes to a model's input would have changed its prediction in a particular way -- have gained considerable traction as a psychologically grounded solution for \gls{XAI}. 
Recent innovations enhance the robustness of automatically generated \glspl{CFE} by introducing the notion of algorithmic plausibility -- i.e. limiting the set of possible counterfactuals to plausible and feasible explanations. %, limiting the set of possible counterfactuals to the training data.
However, the practical benefit of additionally implementing such a constraint on user experience and behavior is yet unclear.
In this study, we evaluate objective and subjective usability of computationally plausible \glspl{CFE} in an iterative learning design targeting novice users.
We rely on a novel, game-like experimental design, revolving around an abstract scenario, suitable for novice users.
Our results show that novice users actually benefit less from receiving computationally plausible rather than closest \glspl{CFE} that simply produce minimal changes leading to the desired outcome.
Responses in a post-game survey reveal no differences in terms of subjective user experience between both groups.
Following the view of psychological plausibility as comparative similarity, this may be explained by the fact that users in the close condition might have experienced their \glspl{CFE} as more psychologically plausible than the computationally plausible counterpart.
In sum, our work highlights a little-considered divergence of definitions of computational plausibility and psychological plausibility, critically confirming the need to incorporate human behavior, preferences and mental models already at the design stages of \gls{XAI} approaches.