
@inproceedings{kulesza_too_2013,
	address = {San Jose, CA, USA},
	title = {Too much, too little, or just right? {Ways} explanations impact end users' mental models},
	isbn = {978-1-4799-0369-6},
	shorttitle = {Too much, too little, or just right?},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6645235},
	doi = {10.1109/VLHCC.2013.6645235},
	urldate = {2020-02-03},
	booktitle = {2013 {IEEE} {Symposium} on {Visual} {Languages} and {Human} {Centric} {Computing}},
	publisher = {IEEE},
	author = {Kulesza, Todd and Stumpf, Simone and Burnett, Margaret and Yang, Sherry and Kwan, Irwin and Wong, Weng-Keen},
	month = sep,
	year = {2013},
	pages = {3--10},
	file = {Akzeptierte Version:/Users/ukuhl/Zotero/storage/9PFEPAMS/Kulesza et al. - 2013 - Too much, too little, or just right Ways explanat.pdf:application/pdf},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	pages = {2825--2830},
}

@misc{artelt_ceml_2019,
	title = {{CEML}: {Counterfactuals} for {Explaining} {Machine} {Learning} models - {A} {Python} toolbox},
	url = {https://www.github.com/andreArtelt/ceml},
	publisher = {GitHub},
	author = {Artelt, Andr{\'e}},
	year = {2019},
	note = {Publication Title: GitHub repository},
}

@article{wachter_counterfactual_2017,
	title = {Counterfactual explanations without opening the black box: {Automated} decisions and the {GDPR}},
	volume = {31},
	journal = {Harv. JL \& Tech.},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	year = {2017},
	note = {Publisher: HeinOnline},
	pages = {841},
}

@article{poyiadzi_face_2019,
	title = {{FACE}: {Feasible} and {Actionable} {Counterfactual} {Explanations}},
	volume = {abs/1909.09369},
	url = {http://arxiv.org/abs/1909.09369},
	journal = {CoRR},
	author = {Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Ra{\'u}l and Bie, Tijl De and Flach, Peter A.},
	year = {2019},
	note = {\_eprint: 1909.09369},
}

@article{looveren_interpretable_2019,
	title = {Interpretable {Counterfactual} {Explanations} {Guided} by {Prototypes}},
	volume = {abs/1907.02584},
	url = {http://arxiv.org/abs/1907.02584},
	journal = {CoRR},
	author = {Looveren, Arnaud Van and Klaise, Janis},
	year = {2019},
	note = {\_eprint: 1907.02584},
}

@article{artelt_computation_2019,
	title = {On the computation of counterfactual explanations - {A} survey},
	volume = {abs/1911.07749},
	url = {http://arxiv.org/abs/1911.07749},
	journal = {CoRR},
	author = {Artelt, Andr{\'e} and Hammer, Barbara},
	year = {2019},
	note = {\_eprint: 1911.07749},
}

@article{karimi_survey_2020,
	title = {A survey of algorithmic recourse: definitions, formulations, solutions, and prospects},
	journal = {arXiv preprint arXiv:2010.04050},
	author = {Karimi, Amir-Hossein and Barthe, Gilles and Sch{\"o}lkopf, Bernhard and Valera, Isabel},
	year = {2020},
}

@article{laugel_issues_2019,
	title = {Issues with post-hoc counterfactual explanations: a discussion},
	volume = {abs/1906.04774},
	url = {http://arxiv.org/abs/1906.04774},
	journal = {CoRR},
	author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Detyniecki, Marcin},
	year = {2019},
	note = {\_eprint: 1906.04774},
}

@article{halpern_causes_2020,
	title = {Causes and explanations: {A} structural-model approach. {Part} {I}: {Causes}},
	journal = {The British journal for the philosophy of science},
	author = {Halpern, Joseph Y and Pearl, Judea},
	year = {2020},
	note = {Publisher: The University of Chicago Press},
}

@inproceedings{lage_human_2019,
	series = {1},
	title = {Human {Evaluation} of {Models} {Built} for {Interpretability}},
	volume = {7},
	abstract = {Recent years have seen a boom in interest in interpretable machine learning systems built on models that can be understood, at least to some degree, by domain experts. However, exactly what kinds of models are truly human-interpretable remains poorly understood. This work advances our understanding of precisely which factors make models interpretable in the context of decision sets, a specific class of logic-based model. We conduct carefully controlled humansubject experiments in two domains across three tasks based on human-simulatability through which we identify specific types of complexity that affect performance more heavily than others{\textendash}trends that are consistent across tasks and domains. These results can inform the choice of regularizers during optimization to learn more interpretable models, and their consistency suggests that there may exist common design principles for interpretable machine learning systems.},
	language = {en},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Human} {Computation} and {Crowdsourcing}},
	author = {Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Samuel J and Doshi-Velez, Finale},
	year = {2019},
	pages = {59--67},
	file = {Lage - Human Evaluation of Models Built for Interpretabil.pdf:/Users/ukuhl/Zotero/storage/8AZYP5I2/Lage - Human Evaluation of Models Built for Interpretabil.pdf:application/pdf},
}

@article{byrne_counterfactual_2016,
	title = {Counterfactual {Thought}},
	volume = {67},
	issn = {0066-4308, 1545-2085},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-122414-033249},
	doi = {10.1146/annurev-psych-122414-033249},
	abstract = {People spontaneously create counterfactual alternatives to reality when they think {\textquotedblleft}if only{\textquotedblright} or {\textquotedblleft}what if{\textquotedblright} and imagine how the past could have been different. The mind computes counterfactuals for many reasons. Counterfactuals explain the past and prepare for the future, they implicate various relations including causal ones, and they affect intentions and decisions. They modulate emotions such as regret and relief, and they support moral judgments such as blame. The loss of the ability to imagine alternatives as a result of injuries to the prefrontal cortex is devastating. The basic cognitive processes that compute counterfactuals mutate aspects of the mental representation of reality to create an imagined alternative, and they compare alternative representations. The ability to create counterfactuals develops throughout childhood and contributes to reasoning about other people{\textquoteright}s beliefs, including their false beliefs. Knowledge affects the plausibility of a counterfactual through the semantic and pragmatic modulation of the mental representation of alternative possibilities.},
	language = {en},
	number = {1},
	urldate = {2019-08-02},
	journal = {Annual Review of Psychology},
	author = {Byrne, Ruth M.J.},
	month = jan,
	year = {2016},
	pages = {135--157},
	file = {Byrne - 2016 - Counterfactual Thought.pdf:/Users/ukuhl/Zotero/storage/4Z9UMBH7/Byrne - 2016 - Counterfactual Thought.pdf:application/pdf},
}

@article{narayanan_how_2018,
	title = {How do {Humans} {Understand} {Explanations} from {Machine} {Learning} {Systems}? {An} {Evaluation} of the {Human}-{Interpretability} of {Explanation}},
	shorttitle = {How do {Humans} {Understand} {Explanations} from {Machine} {Learning} {Systems}?},
	url = {http://arxiv.org/abs/1802.00682},
	abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
	language = {en},
	urldate = {2019-06-04},
	journal = {arXiv:1802.00682 [cs]},
	author = {Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Narayanan et al. - 2018 - How do Humans Understand Explanations from Machine.pdf:/Users/ukuhl/Zotero/storage/WZ5M5H36/Narayanan et al. - 2018 - How do Humans Understand Explanations from Machine.pdf:application/pdf},
}

@article{doshi-velez_towards_2017,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	language = {en},
	journal = {arXiv:1702.08608},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = feb,
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:/Users/ukuhl/Zotero/storage/CLGYTP36/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:application/pdf},
}

@inproceedings{stepin_paving_2019,
	address = {Tokyo, Japan},
	title = {Paving the way towards counterfactual generation in argumentative conversational agents},
	url = {https://www.aclweb.org/anthology/W19-8405},
	doi = {10.18653/v1/W19-8405},
	abstract = {Counterfactual explanations present an effective way to interpret predictions of black-box machine learning algorithms. Whereas there is a significant body of research on counterfactual reasoning in philosophy and theoretical computer science, little attention has been paid to counterfactuals in regard to their explanatory capacity. In this paper, we review methods of argumentation theory and natural language generation that counterfactual explanation generation could benefit from most and discuss prospective directions for further research on counterfactual generation in explainable Artificial Intelligence.},
	language = {en},
	urldate = {2020-01-21},
	booktitle = {Proceedings of the 1st {Workshop} on {Interactive} {Natural} {Language} {Technology} for {Explainable} {Artificial} {Intelligence} ({NL4XAI} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Stepin, Ilia and Catala, Alejandro and Pereira-Fari{\~n}a, Martin and Alonso, Jose M.},
	year = {2019},
	pages = {20--25},
	file = {Stepin et al. - 2019 - Paving the way towards counterfactual generation i.pdf:/Users/ukuhl/Zotero/storage/PQKQZ2W7/Stepin et al. - 2019 - Paving the way towards counterfactual generation i.pdf:application/pdf},
}

@article{adadi_peeking_2018,
	title = {Peeking {Inside} the {Black}-{Box}: {A} {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Peeking {Inside} the {Black}-{Box}},
	doi = {10.1109/ACCESS.2018.2870052},
	abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
	journal = {IEEE Access},
	author = {Adadi, Amina and Berrada, Mohammed},
	year = {2018},
	keywords = {AI-based systems, artificial intelligence, Biological system modeling, black-box models, black-box nature, Conferences, explainable AI, explainable artificial intelligence, Explainable artificial intelligence, fourth industrial revolution, interpretable machine learning, Machine learning, Machine learning algorithms, Market research, Prediction algorithms, XAI},
	pages = {52138--52160},
	file = {IEEE Xplore Abstract Record:/Users/ukuhl/Zotero/storage/W67P7I9I/8466590.html:text/html;IEEE Xplore Full Text PDF:/Users/ukuhl/Zotero/storage/3E4VHELQ/Adadi und Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf:application/pdf},
}

@article{epstude_functional_2008,
	title = {The {Functional} {Theory} of {Counterfactual} {Thinking}},
	volume = {12},
	issn = {1088-8683},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2408534/},
	doi = {10.1177/1088868308316091},
	abstract = {Counterfactuals are thoughts about alternatives to past events, that is, thoughts of what might have been. This article provides an updated account of the functional theory of counterfactual thinking, suggesting that such thoughts are best explained in terms of their role in behavior regulation and performance improvement. The article reviews a wide range of cognitive experiments indicating that counterfactual thoughts may influence behavior by either of two routes: a content-specific pathway (which involves specific informational effects on behavioral intentions, which then influence behavior) and a content-neutral pathway (which involves indirect effects via affect, mind-sets, or motivation). The functional theory is particularly useful in organizing recent findings regarding counterfactual thinking and mental health. The article concludes by considering the connections to other theoretical conceptions, especially recent advances in goal cognition.},
	number = {2},
	journal = {Personality and social psychology review : an official journal of the Society for Personality and Social Psychology, Inc},
	author = {Epstude, Kai and Roese, Neal J.},
	month = may,
	year = {2008},
	pmid = {18453477},
	pmcid = {PMC2408534},
	pages = {168--192},
	file = {PubMed Central Full Text PDF:/Users/ukuhl/Zotero/storage/AUCJ2GQV/Epstude und Roese - 2008 - The Functional Theory of Counterfactual Thinking.pdf:application/pdf},
}

@inproceedings{offert_i_2017,
	address = {Long Beach, CA, USA},
	title = {"{I} know it when {I} see it". {Visualization} and {Intuitive} {Interpretability}},
	url = {http://arxiv.org/abs/1711.08042},
	abstract = {Most research on the interpretability of machine learning systems focuses on the development of a more rigorous notion of interpretability. I suggest that a better understanding of the deficiencies of the intuitive notion of interpretability is needed as well. I show that visualization enables but also impedes intuitive interpretability, as it presupposes two levels of technical pre-interpretation: dimensionality reduction and regularization. Furthermore, I argue that the use of positive concepts to emulate the distributed semantic structure of machine learning models introduces a significant human bias into the model. As a consequence, I suggest that, if intuitive interpretability is needed, singular representations of internal model states should be avoided.},
	booktitle = {{arXiv}:1711.08042 [stat]},
	author = {Offert, Fabian},
	month = dec,
	year = {2017},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/ANUIAQX5/Offert - 2017 - I know it when I see it. Visualization and Intui.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/KZHL5SK7/1711.html:text/html},
}

@article{goldinger_blaming_2003,
	title = {"{Blaming} {The} {Victim}" {Under} {Memory} {Load}},
	volume = {14},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1111/1467-9280.01423},
	doi = {10.1111/1467-9280.01423},
	abstract = {When presented with negative outcomes, people often engage in counterfactual thinking, imagining various ways that events might have been different. This appears to be a spontaneous behavior, with considerable adaptive value. Nevertheless, counterfactual thinking may also engender systematic biases in various judgment tasks, such as allocating blame for a mishap, or deciding on the appropriate compensation to a victim. Thus, counterfactuals sometimes require thought suppression or discounting, potentially resource-demanding tasks. In this study, participants made mock-jury decisions about control and counterfactual versions of simple stories. The judgments of two groups of participants, differing in their respective levels of working memory capacity, were compared. In addition, all participants held memory loads during various stages of the primary task. Lower-span individuals were especially susceptible to bias associated with the counterfactual manipulation, but only when holding memory loads during judgment. The results suggest that counterfactual thoughts arise automatically, and may later require effortful, capacity-demanding suppression.},
	language = {en},
	number = {1},
	urldate = {2020-02-24},
	journal = {Psychological Science},
	author = {Goldinger, Stephen D. and Kleider, Heather M. and Azuma, Tamiko and Beike, Denise R.},
	month = jan,
	year = {2003},
	pages = {81--85},
}

@article{sanna_antecedents_1996,
	title = {Antecedents to {Spontaneous} {Counterfactual} {Thinking}: {Effects} of {Expectancy} {Violation} and {Outcome} {Valence}},
	volume = {22},
	issn = {0146-1672},
	shorttitle = {Antecedents to {Spontaneous} {Counterfactual} {Thinking}},
	url = {https://doi.org/10.1177/0146167296229005},
	doi = {10.1177/0146167296229005},
	abstract = {Three studies examined the effects of expectancy violation and outcome valence on spontaneous counterfactual thinking. In Study 1, prior expectations and outcome valence were varied orthogonally in a vignette. More counterfactuals were generated after failures and unexpected outcomes. Also, more additive than subtractive counterfatuals were found after failure, particularly unexpected failure, and more subtractive than additive counterfactuals were found after unexpected success. Evidence for the generality of these results was obtained in Study 2, in which counterfactuals were assessed after students' real-life exam performances. In Study 3, the authors further assessed nonspontaneous counterfactuals, which were shown to differ in number and structure from spontaneous counterfactuals. Discussion centers around antecedents to spontaneous counterfactual thinking and comparisons to research on spontaneous causal attributions.},
	number = {9},
	urldate = {2020-02-24},
	journal = {Personality and Social Psychology Bulletin},
	author = {Sanna, Lawrence J. and Turley, Kandi Jo},
	month = sep,
	year = {1996},
	pages = {906--919},
	file = {SAGE PDF Full Text:/Users/ukuhl/Zotero/storage/C7SWFBXA/Sanna und Turley - 1996 - Antecedents to Spontaneous Counterfactual Thinking.pdf:application/pdf},
}

@incollection{kahneman_simulation_1982,
	edition = {1},
	title = {The simulation heuristic},
	isbn = {978-0-521-28414-1 978-0-521-24064-2 978-0-511-80947-7},
	url = {https://www.cambridge.org/core/product/identifier/CBO9780511809477A026/type/book_part},
	urldate = {2020-02-25},
	booktitle = {Judgment under {Uncertainty}},
	publisher = {Cambridge University Press},
	author = {Kahneman, Daniel and Tversky, Amos},
	editor = {Kahneman, Daniel and Slovic, Paul and Tversky, Amos},
	month = apr,
	year = {1982},
	doi = {10.1017/CBO9780511809477.015},
	pages = {201--208},
	file = {Kahneman und Tversky - 1982 - The simulation heuristic.pdf:/Users/ukuhl/Zotero/storage/33KLDJ4B/Kahneman und Tversky - 1982 - The simulation heuristic.pdf:application/pdf},
}

@book{heider_psychology_1958,
	address = {New York, NY, US},
	title = {The psychology of interpersonal relations},
	publisher = {John Wiley \& Sons Ltd.},
	author = {Heider, Fritz},
	year = {1958},
}

@misc{artelt_counterfactual_2020,
	title = {Counterfactual explanations of {ML} models},
	abstract = {The increasing use of machine learning in practice and legal regulations like EU{\textquoteright}s GDPR cause the necessity to be able to explain the prediction and behavior of machine learning models. A prominent example of particularly intuitive explanations of AI models in the context of decision making are counterfactual explanations. Yet, it is still an open research problem how to efficiently compute counterfactual explanations for many models. We develop algorithms for efficiently computing counterfactual explanations of white-box models.},
	author = {Artelt, Andr{\'e}},
	month = jan,
	year = {2020},
	note = {Place: Bielefeld},
}

@article{de_brigard_coming_2013,
	title = {Coming to {Grips} {With} the {Past}: {Effect} of {Repeated} {Simulation} on the {Perceived} {Plausibility} of {Episodic} {Counterfactual} {Thoughts}},
	volume = {24},
	issn = {0956-7976},
	shorttitle = {Coming to {Grips} {With} the {Past}},
	url = {https://doi.org/10.1177/0956797612468163},
	doi = {10.1177/0956797612468163},
	abstract = {When people revisit previous experiences, they often engage in episodic counterfactual thinking: mental simulations of alternative ways in which personal past events could have occurred. The present study employed a novel experimental paradigm to examine the influence of repeated simulation on the perceived plausibility of upward, downward, and neutral episodic counterfactual thoughts. Participants were asked to remember negative, positive, and neutral autobiographical memories. One week later, they self-generated upward, downward, and neutral counterfactual alternatives to those memories. The following day, they resimulated each of those counterfactuals either once or four times. The results indicate that repeated simulation of upward, downward, and neutral episodic counterfactual events decreases their perceived plausibility while increasing ratings of the ease, detail, and valence of the simulations. This finding suggests a difference between episodic counterfactual thoughts and other kinds of self-referential simulations. Possible implications of this finding for pathological and nonpathological anxiety are discussed.},
	number = {7},
	urldate = {2020-03-02},
	journal = {Psychological Science},
	author = {De Brigard, Felipe and Szpunar, Karl K. and Schacter, Daniel L.},
	month = jul,
	year = {2013},
	pages = {1329--1334},
	file = {SAGE PDF Full Text:/Users/ukuhl/Zotero/storage/K585BVLD/De Brigard et al. - 2013 - Coming to Grips With the Past Effect of Repeated .pdf:application/pdf},
}

@article{holzinger_measuring_2020,
	title = {Measuring the {Quality} of {Explanations}: {The} {System} {Causability} {Scale} ({SCS}): {Comparing} {Human} and {Machine} {Explanations}},
	volume = {34},
	issn = {0933-1875, 1610-1987},
	shorttitle = {Measuring the {Quality} of {Explanations}},
	url = {http://link.springer.com/10.1007/s13218-020-00636-z},
	doi = {10.1007/s13218-020-00636-z},
	abstract = {Recent success in Artificial Intelligence (AI) and Machine Learning (ML) allow problem solving automatically without any human intervention. Autonomous approaches can be very convenient. However, in certain domains, e.g., in the medical domain, it is necessary to enable a domain expert to understand, why an algorithm came up with a certain result. Consequently, the field of Explainable AI (xAI) rapidly gained interest worldwide in various domains, particularly in medicine. Explainable AI studies transparency and traceability of opaque AI/ML and there are already a huge variety of methods. For example with layer-wise relevance propagation relevant parts of inputs to, and representations in, a neural network which caused a result, can be highlighted. This is a first important step to ensure that end users, e.g., medical professionals, assume responsibility for decision making with AI/ML and of interest to professionals and regulators. Interactive ML adds the component of human expertise to AI/ML processes by enabling them to re-enact and retrace AI/ML results, e.g. let them check it for plausibility. This requires new human{\textendash}AI interfaces for explainable AI. In order to build effective and efficient interactive human{\textendash}AI interfaces we have to deal with the question of how to evaluate the quality of explanations given by an explainable AI system. In this paper we introduce our System Causability Scale to measure the quality of explanations. It is based on our notion of Causability (Holzinger et al. in Wiley Interdiscip Rev Data Min Knowl Discov 9(4), 2019) combined with concepts adapted from a widely-accepted usability scale.},
	language = {en},
	number = {2},
	urldate = {2020-05-25},
	journal = {KI - K{\"u}nstliche Intelligenz},
	author = {Holzinger, Andreas and Carrington, Andr{\'e} and M{\"u}ller, Heimo},
	month = jan,
	year = {2020},
	pages = {193--198},
	file = {Holzinger et al. - 2020 - Measuring the Quality of Explanations The System .pdf:/Users/ukuhl/Zotero/storage/EMUDKVCC/Holzinger et al. - 2020 - Measuring the Quality of Explanations The System .pdf:application/pdf},
}

@article{keane_if_2021,
	title = {If {Only} {We} {Had} {Better} {Counterfactual} {Explanations}: {Five} {Key} {Deficits} to {Rectify} in the {Evaluation} of {Counterfactual} {XAI} {Techniques}},
	shorttitle = {If {Only} {We} {Had} {Better} {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/2103.01035},
	abstract = {In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI). These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21\% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.},
	journal = {arXiv:2103.01035 [cs]},
	author = {Keane, Mark T. and Kenny, Eoin M. and Delaney, Eoin and Smyth, Barry},
	month = feb,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/7S2HMAX4/Keane et al. - 2021 - If Only We Had Better Counterfactual Explanations.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/6TASU892/2103.html:text/html},
}

@inproceedings{lim_why_2009,
	address = {Boston MA USA},
	title = {\textit{{Why}} and why not explanations improve the intelligibility of context-aware intelligent systems},
	isbn = {978-1-60558-246-7},
	url = {https://dl.acm.org/doi/10.1145/1518701.1519023},
	doi = {10.1145/1518701.1519023},
	abstract = {Context-aware intelligent systems employ implicit inputs, and make decisions based on complex rules and machine learning models that are rarely clear to users. Such lack of system intelligibility can lead to loss of user trust, satisfaction and acceptance of these systems. However, automatically providing explanations about a system{\quotedblbase}s decision process can help mitigate this problem. In this paper we present results from a controlled study with over 200 participants in which the effectiveness of different types of explanations was examined. Participants were shown examples of a system{\quotedblbase}s operation along with various automatically generated explanations, and then tested on their understanding of the system. We show, for example, that explanations describing why the system behaved a certain way resulted in better understanding and stronger feelings of trust. Explanations describing why the system did not behave a certain way, resulted in lower understanding yet adequate performance. We discuss implications for the use of our findings in real-world context-aware applications.},
	language = {en},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lim, Brian Y. and Dey, Anind K. and Avrahami, Daniel},
	month = apr,
	year = {2009},
	pages = {2119--2128},
	file = {Lim et al. - 2009 - Why and why not explanations improve the in.pdf:/Users/ukuhl/Zotero/storage/3EDWH2TP/Lim et al. - 2009 - Why and why not explanations improve the in.pdf:application/pdf},
}

@inproceedings{dodge_explaining_2019,
	address = {Marina del Ray California},
	title = {Explaining models: an empirical study of how explanations impact fairness judgment},
	isbn = {978-1-4503-6272-6},
	shorttitle = {Explaining models},
	url = {https://dl.acm.org/doi/10.1145/3301275.3302310},
	doi = {10.1145/3301275.3302310},
	language = {en},
	urldate = {2021-08-04},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {ACM},
	author = {Dodge, Jonathan and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel K. E. and Dugan, Casey},
	month = mar,
	year = {2019},
	pages = {275--285},
	file = {Eingereichte Version:/Users/ukuhl/Zotero/storage/9T3AW4FR/Dodge et al. - 2019 - Explaining models an empirical study of how expla.pdf:application/pdf},
}

@article{verma_counterfactual_2020,
	title = {Counterfactual {Explanations} for {Machine} {Learning}: {A} {Review}},
	shorttitle = {Counterfactual {Explanations} for {Machine} {Learning}},
	url = {http://arxiv.org/abs/2010.10596},
	abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.},
	journal = {arXiv:2010.10596 [cs, stat]},
	author = {Verma, Sahil and Dickerson, John and Hines, Keegan},
	month = oct,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/N4TY4P8D/Verma et al. - 2020 - Counterfactual Explanations for Machine Learning .pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/KPZJFDGC/2010.html:text/html},
}

@article{ehsan_who_2021,
	title = {The {Who} in {Explainable} {AI}: {How} {AI} {Background} {Shapes} {Perceptions} of {AI} {Explanations}},
	shorttitle = {The {Who} in {Explainable} {AI}},
	url = {http://arxiv.org/abs/2107.13509},
	abstract = {Explainability of AI systems is critical for users to take informed actions and hold systems accountable. While "opening the opaque box" is important, understanding who opens the box can govern if the Human-AI interaction is effective. In this paper, we conduct a mixed-methods study of how two different groups of whos{\textendash}people with and without a background in AI{\textendash}perceive different types of AI explanations. These groups were chosen to look at how disparities in AI backgrounds can exacerbate the creator-consumer gap. We quantitatively share what the perceptions are along five dimensions: confidence, intelligence, understandability, second chance, and friendliness. Qualitatively, we highlight how the AI background influences each group's interpretations and elucidate why the differences might exist through the lenses of appropriation and cognitive heuristics. We find that (1) both groups had unwarranted faith in numbers, to different extents and for different reasons, (2) each group found explanatory values in different explanations that went beyond the usage we designed them for, and (3) each group had different requirements of what counts as humanlike explanations. Using our findings, we discuss potential negative consequences such as harmful manipulation of user trust and propose design interventions to mitigate them. By bringing conscious awareness to how and why AI backgrounds shape perceptions of potential creators and consumers in XAI, our work takes a formative step in advancing a pluralistic Human-centered Explainable AI discourse.},
	journal = {arXiv:2107.13509 [cs]},
	author = {Ehsan, Upol and Passi, Samir and Liao, Q. Vera and Chan, Larry and Lee, I.-Hsiang and Muller, Michael and Riedl, Mark O.},
	month = jul,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/BQS2YWT2/Ehsan et al. - 2021 - The Who in Explainable AI How AI Background Shape.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/QQ68YGQ3/2107.html:text/html},
}

@article{van_der_waa_evaluating_2021,
	title = {Evaluating {XAI}: {A} comparison of rule-based and example-based explanations},
	volume = {291},
	issn = {00043702},
	shorttitle = {Evaluating {XAI}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370220301533},
	doi = {10.1016/j.artint.2020.103404},
	language = {en},
	urldate = {2021-08-10},
	journal = {Artificial Intelligence},
	author = {van der Waa, Jasper and Nieuwburg, Elisabeth and Cremers, Anita and Neerincx, Mark},
	month = feb,
	year = {2021},
	pages = {103404},
	file = {Volltext:/Users/ukuhl/Zotero/storage/XBZC9WIA/van der Waa et al. - 2021 - Evaluating XAI A comparison of rule-based and exa.pdf:application/pdf},
}

@article{guidotti_survey_2019,
	title = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
	volume = {51},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3236009},
	doi = {10.1145/3236009},
	abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
	language = {en},
	number = {5},
	journal = {ACM Computing Surveys},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	month = jan,
	year = {2019},
	pages = {1--42},
	file = {Eingereichte Version:/Users/ukuhl/Zotero/storage/I26LGY5T/Guidotti et al. - 2019 - A Survey of Methods for Explaining Black Box Model.pdf:application/pdf},
}

@article{lipton_contrastive_1990,
	title = {Contrastive {Explanation}},
	volume = {27},
	issn = {1358-2461, 1755-3555},
	url = {https://www.cambridge.org/core/product/identifier/S1358246100005130/type/journal_article},
	doi = {10.1017/S1358246100005130},
	abstract = {According to a causal model of explanation, we explain phenomena by giving their causes or, where the phenomena are themselves causal regularities, we explain them by giving a mechanism linking cause and effect. If we explain why smoking causes cancer, we do not give the cause of this causal connection, but we do give the causal mechanism that makes it. The claim that to explain is to give a cause is not only natural and plausible, but it also avoids many of the objections to other accounts of explanation, such as the views that to explain is to give a reason to believe the phenomenon occurred, to somehow make the phenomenon familiar, or to give a Deductive-Nomological argument. Unlike the reason for belief account, a causal model makes a clear distinction between understanding why a phenomenon occurs and merely knowing that it does, and the model does so in a way that makes understanding unmysterious and objective. Understanding is not some sort of super-knowledge, but simply more knowledge: knowledge of the phenomenon and knowledge of its causal history. A causal model makes it clear how something can explain without itself being explained, and so avoids the regress of whys, since we can know a phenomenon's cause without knowing the cause of the cause. It also accounts for legitimate self-evidencing explanations, explanations where the phenomenon is an essential part of the evidence that the explanation is correct, so the explanation can not supply a non-circular reason for believing the phenomenon occurred. There is no barrier to knowing a cause through its effects and also knowing that it is their cause. The speed of recession of a star explains its observed red-shift, even though the shift is an essential part of the evidence for its speed of recession. The model also avoids the most serious objection to the familiarity view, which is that some phenomena are familiar yet not understood, since a phenomenon can be perfectly familiar, such as the blueness of the sky or the fact that the same side of the moon always faces the earth, even if we do not know its cause. Finally, a causal model avoids many of the objections to the Deductive-Nomological model. Ordinary explanations do not have to meet the requirements of the Deductive-Nomological model, because one does not need to give a law to give a cause, and one does not need to know a law to have good reason to believe that a cause is a cause. As for the notorious over-permissiveness of the Deductive-Nomological model, the reason recession explains red-shift but not conversely, is simply that causes explain effects but not conversely, and the reason a conjunction of laws does not explain its conjuncts is that conjunctions do not cause their conjuncts.},
	language = {en},
	urldate = {2021-08-13},
	journal = {Royal Institute of Philosophy Supplement},
	author = {Lipton, Peter},
	month = mar,
	year = {1990},
	pages = {247--266},
	file = {Lipton - 1990 - Contrastive Explanation.pdf:/Users/ukuhl/Zotero/storage/Z963NVIX/Lipton - 1990 - Contrastive Explanation.pdf:application/pdf},
}

@incollection{lombrozo_explanation_2012,
	title = {Explanation and {Abductive} {Inference}},
	url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780199734689.001.0001/oxfordhb-9780199734689-e-14},
	urldate = {2021-08-13},
	booktitle = {The {Oxford} {Handbook} of {Thinking} and {Reasoning}},
	publisher = {Oxford University Press},
	author = {Lombrozo, Tania},
	editor = {Holyoak, Keith J. and Morrison, Robert G.},
	month = mar,
	year = {2012},
	doi = {10.1093/oxfordhb/9780199734689.013.0014},
	pages = {260--276},
	file = {Lombrozo - 2012 - Explanation and Abductive Inference.pdf:/Users/ukuhl/Zotero/storage/UBDU978U/Lombrozo - 2012 - Explanation and Abductive Inference.pdf:application/pdf},
}

@article{hilton_knowledge-based_1986,
	title = {Knowledge-based causal attribution: {The} abnormal conditions focus model.},
	volume = {93},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Knowledge-based causal attribution},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.93.1.75},
	doi = {10.1037/0033-295X.93.1.75},
	language = {en},
	number = {1},
	urldate = {2021-08-13},
	journal = {Psychological Review},
	author = {Hilton, Denis J. and Slugoski, Ben R.},
	year = {1986},
	pages = {75--88},
	file = {Hilton und Slugoski - 1986 - Knowledge-based causal attribution The abnormal c.pdf:/Users/ukuhl/Zotero/storage/FXT2TYJS/Hilton und Slugoski - 1986 - Knowledge-based causal attribution The abnormal c.pdf:application/pdf},
}

@article{roese_counterfactual_1997,
	title = {Counterfactual thinking.},
	volume = {121},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.121.1.133},
	doi = {10.1037/0033-2909.121.1.133},
	language = {en},
	number = {1},
	urldate = {2021-08-13},
	journal = {Psychological Bulletin},
	author = {Roese, Neal J.},
	year = {1997},
	pages = {133--148},
	file = {Roese - 1997 - Counterfactual thinking..pdf:/Users/ukuhl/Zotero/storage/XHVGPZ2R/Roese - 1997 - Counterfactual thinking..pdf:application/pdf},
}

@incollection{roese_functional_2017,
	title = {The {Functional} {Theory} of {Counterfactual} {Thinking}: {New} {Evidence}, {New} {Challenges}, {New} {Insights}},
	volume = {56},
	isbn = {978-0-12-812120-7},
	shorttitle = {The {Functional} {Theory} of {Counterfactual} {Thinking}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0065260117300187},
	language = {en},
	urldate = {2021-08-16},
	booktitle = {Advances in {Experimental} {Social} {Psychology}},
	publisher = {Elsevier},
	author = {Roese, Neal J. and Epstude, Kai},
	year = {2017},
	doi = {10.1016/bs.aesp.2017.02.001},
	pages = {1--79},
	file = {Volltext:/Users/ukuhl/Zotero/storage/FXRVS769/Roese und Epstude - 2017 - The Functional Theory of Counterfactual Thinking .pdf:application/pdf},
}

@article{roese_counterfactual_1999,
	title = {Counterfactual thinking and regulatory focus: {Implications} for action versus inaction and sufficiency versus necessity.},
	volume = {77},
	issn = {1939-1315, 0022-3514},
	shorttitle = {Counterfactual thinking and regulatory focus},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.77.6.1109},
	doi = {10.1037/0022-3514.77.6.1109},
	language = {en},
	number = {6},
	urldate = {2021-08-16},
	journal = {Journal of Personality and Social Psychology},
	author = {Roese, Neal J. and Hur, Taekyun and Pennington, Ginger L.},
	year = {1999},
	pages = {1109--1120},
}

@article{alicke_culpable_2008,
	title = {Culpable {Control} and {Counterfactual} {Reasoning} in the {Psychology} of {Blame}},
	volume = {34},
	issn = {0146-1672, 1552-7433},
	url = {http://journals.sagepub.com/doi/10.1177/0146167208321594},
	doi = {10.1177/0146167208321594},
	abstract = {Many counterfactual reasoning studies assess how people ascribe blame for harmful actions. By itself, the knowledge that a harmful outcome could easily have been avoided does not predict blame. In three studies, the authors showed that an outcome's mutability influences blame and related judgments when it is coupled with a basis for negative evaluations. Study 1 showed that mutability influenced blame and compensation judgments when a physician was negligent but not when the physician took reasonable precautions to prevent harm. Study 2 showed that this finding was attenuated when the victim contributed to his own demise. In Study 3, whether an actor just missed arriving on time to see his dying mother or had no chance to see her influenced his blameworthiness when his reason for being late provided a basis for negative evaluations but made no difference when there was a positive reason for the delay. These findings clarify the conditions under which an outcome's mutability is likely to influence blame and related attributions.},
	language = {en},
	number = {10},
	urldate = {2021-08-16},
	journal = {Personality and Social Psychology Bulletin},
	author = {Alicke, Mark D. and Buckingham, Justin and Zell, Ethan and Davis, Teresa},
	month = oct,
	year = {2008},
	pages = {1371--1381},
	file = {Alicke et al. - 2008 - Culpable Control and Counterfactual Reasoning in t.pdf:/Users/ukuhl/Zotero/storage/6BTIBZYM/Alicke et al. - 2008 - Culpable Control and Counterfactual Reasoning in t.pdf:application/pdf},
}

@article{markman_reflection_2003,
	title = {A {Reflection} and {Evaluation} {Model} of {Comparative} {Thinking}},
	volume = {7},
	issn = {1088-8683, 1532-7957},
	url = {http://journals.sagepub.com/doi/10.1207/S15327957PSPR0703_04},
	doi = {10.1207/S15327957PSPR0703_04},
	abstract = {This article reviews research on counterfactual, social, and temporal comparisons and proposes a Reflection and Evaluation Model (REM) as an organizing framework. At the heart of the model is the assertion that 2 psychologically distinct modes of mental simulation operate during comparative thinking: reflection, an experiential ({\textquotedblleft}as if{\textquotedblright}) mode of thinking characterized by vividly simulating that information about the comparison standard is true of, or part of, the self; and evaluation, an evaluative mode of thinking characterized by the use of information about the standard as a reference point against which to evaluate one's present standing. Reflection occurs when information about the standard is included in one's self-construal, and evaluation occurs when such information is excluded. The result of reflection is that standard-consistent cognitions about the self become highly accessible, thereby yielding affective assimilation; whereas the result of evaluation is that comparison information is used as a standard against which one's present standing is evaluated, thereby yielding affective contrast. The resulting affect leads to either an increase or decrease in behavioral persistence as a function of the type of task with which one is engaged, and a combination of comparison-derived causal inferences and regulatory focus strategies direct one toward adopting specific future action plans.},
	language = {en},
	number = {3},
	urldate = {2021-08-16},
	journal = {Personality and Social Psychology Review},
	author = {Markman, Keith D. and McMullen, Matthew N.},
	month = aug,
	year = {2003},
	pages = {244--267},
}

@inproceedings{dandl_multi-objective_2020,
	address = {Cham},
	title = {Multi-{Objective} {Counterfactual} {Explanations}},
	volume = {12269},
	isbn = {978-3-030-58111-4 978-3-030-58112-1},
	url = {http://link.springer.com/10.1007/978-3-030-58112-1_31},
	doi = {10.1007/978-3-030-58112-1_31},
	language = {en},
	urldate = {2021-08-16},
	booktitle = {International {Conference} on {Parallel} {Problem} {Solving} from {Nature}},
	publisher = {Springer International Publishing},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	year = {2020},
	pages = {448--469},
	file = {Volltext:/Users/ukuhl/Zotero/storage/4NPKM7WQ/Dandl et al. - 2020 - Multi-Objective Counterfactual Explanations.pdf:application/pdf},
}

@inproceedings{le_grace_2020,
	address = {Virtual Event CA USA},
	title = {{GRACE}: {Generating} {Concise} and {Informative} {Contrastive} {Sample} to {Explain} {Neural} {Network} {Model}'s {Prediction}},
	isbn = {978-1-4503-7998-4},
	shorttitle = {{GRACE}},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403066},
	doi = {10.1145/3394486.3403066},
	language = {en},
	urldate = {2021-08-16},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Le, Thai and Wang, Suhang and Lee, Dongwon},
	month = aug,
	year = {2020},
	pages = {238--248},
	file = {Eingereichte Version:/Users/ukuhl/Zotero/storage/X9SVRD4R/Le et al. - 2020 - GRACE Generating Concise and Informative Contrast.pdf:application/pdf},
}

@article{guidotti_local_2018,
	title = {Local {Rule}-{Based} {Explanations} of {Black} {Box} {Decision} {Systems}},
	url = {http://arxiv.org/abs/1805.10820},
	abstract = {The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of machine learning components in socially sensitive and safety-critical contexts. \%Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.},
	urldate = {2021-08-16},
	journal = {arXiv:1805.10820 [cs]},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
	month = may,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/WCLUSE53/Guidotti et al. - 2018 - Local Rule-Based Explanations of Black Box Decisio.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/TALKXILN/1805.html:text/html},
}

@incollection{artelt_convex_2020,
	address = {Cham},
	title = {Convex {Density} {Constraints} for {Computing} {Plausible} {Counterfactual} {Explanations}},
	volume = {12396},
	isbn = {978-3-030-61608-3},
	url = {https://link.springer.com/10.1007/978-3-030-61609-0_28},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} {\textendash} {ICANN} 2020},
	publisher = {Springer International Publishing},
	author = {Artelt, Andr{\'e} and Hammer, Barbara},
	editor = {Farka{\v s}, Igor and Masulli, Paolo and Wermter, Stefan},
	year = {2020},
	doi = {10.1007/978-3-030-61609-0_28},
	pages = {353--365},
	file = {Eingereichte Version:/Users/ukuhl/Zotero/storage/5JNBEAJS/Artelt und Hammer - 2020 - Convex Density Constraints for Computing Plausible.pdf:application/pdf},
}

@article{bayer_role_2021,
	title = {The role of domain expertise in trusting and following explainable {AI} decision support systems},
	issn = {1246-0125, 2116-7052},
	url = {https://www.tandfonline.com/doi/full/10.1080/12460125.2021.1958505},
	doi = {10.1080/12460125.2021.1958505},
	language = {en},
	urldate = {2021-08-23},
	journal = {Journal of Decision Systems},
	author = {Bayer, Sarah and Gimpel, Henner and Markgraf, Moritz},
	month = aug,
	year = {2021},
	pages = {1--29},
	file = {Bayer et al. - 2021 - The role of domain expertise in trusting and follo.pdf:/Users/ukuhl/Zotero/storage/FCWHY77F/Bayer et al. - 2021 - The role of domain expertise in trusting and follo.pdf:application/pdf},
}

@article{hoffman_metrics_2019,
	title = {Metrics for {Explainable} {AI}: {Challenges} and {Prospects}},
	shorttitle = {Metrics for {Explainable} {AI}},
	url = {http://arxiv.org/abs/1812.04608},
	abstract = {The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
	urldate = {2021-08-24},
	journal = {arXiv:1812.04608 [cs]},
	author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
	month = feb,
	year = {2019},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/BLBRBW9X/Hoffman et al. - 2019 - Metrics for Explainable AI Challenges and Prospec.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/S8TJ4ZTB/1812.html:text/html},
}

@article{chi_icap_2014,
	title = {The {ICAP} {Framework}: {Linking} {Cognitive} {Engagement} to {Active} {Learning} {Outcomes}},
	volume = {49},
	issn = {0046-1520, 1532-6985},
	shorttitle = {The {ICAP} {Framework}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00461520.2014.965823},
	doi = {10.1080/00461520.2014.965823},
	language = {en},
	number = {4},
	urldate = {2021-08-25},
	journal = {Educational Psychologist},
	author = {Chi, Michelene T. H. and Wylie, Ruth},
	month = oct,
	year = {2014},
	pages = {219--243},
}

@article{akula_cocox_2020,
	title = {{CoCoX}: {Generating} {Conceptual} and {Counterfactual} {Explanations} via {Fault}-{Lines}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{CoCoX}},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/5643},
	doi = {10.1609/aaai.v34i03.5643},
	abstract = {We present CoCoX (short for Conceptual and Counterfactual Explanations), a model for explaining decisions made by a deep convolutional neural network (CNN). In Cognitive Psychology, the factors (or semantic-level features) that humans zoom in on when they imagine an alternative to a model prediction are often referred to as fault-lines. Motivated by this, our CoCoX model explains decisions made by a CNN using fault-lines. Specifically, given an input image I for which a CNN classification model M predicts class cpred, our fault-line based explanation identifies the minimal semantic-level features (e.g., stripes on zebra, pointed ears of dog), referred to as explainable concepts, that need to be added to or deleted from I in order to alter the classification category of I by M to another specified class calt. We argue that, due to the conceptual and counterfactual nature of fault-lines, our CoCoX explanations are practical and more natural for both expert and non-expert users to understand the internal workings of complex deep learning models. Extensive quantitative and qualitative experiments verify our hypotheses, showing that CoCoX significantly outperforms the state-of-the-art explainable AI models. Our implementation is available at https://github.com/arjunakula/CoCoX},
	number = {03},
	urldate = {2021-08-25},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Akula, Arjun and Wang, Shuai and Zhu, Song-Chun},
	month = apr,
	year = {2020},
	pages = {2594--2601},
	file = {Volltext:/Users/ukuhl/Zotero/storage/FIYY99R7/Akula et al. - 2020 - CoCoX Generating Conceptual and Counterfactual Ex.pdf:application/pdf},
}

@incollection{keane_good_2020,
	address = {Cham},
	title = {Good {Counterfactuals} and {Where} to {Find} {Them}: {A} {Case}-{Based} {Technique} for {Generating} {Counterfactuals} for {Explainable} {AI} ({XAI})},
	volume = {12311},
	isbn = {978-3-030-58341-5 978-3-030-58342-2},
	shorttitle = {Good {Counterfactuals} and {Where} to {Find} {Them}},
	url = {http://link.springer.com/10.1007/978-3-030-58342-2_11},
	language = {en},
	urldate = {2021-08-25},
	booktitle = {Case-{Based} {Reasoning} {Research} and {Development}},
	publisher = {Springer International Publishing},
	author = {Keane, Mark T. and Smyth, Barry},
	editor = {Watson, Ian and Weber, Rosina},
	year = {2020},
	doi = {10.1007/978-3-030-58342-2_11},
	pages = {163--178},
	file = {Eingereichte Version:/Users/ukuhl/Zotero/storage/2JQA8GTK/Keane und Smyth - 2020 - Good Counterfactuals and Where to Find Them A Cas.pdf:application/pdf},
}

@article{dahlback_wizard_1993,
	title = {Wizard of {Oz} studies {\textemdash} why and how},
	volume = {6},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/095070519390017N},
	doi = {10.1016/0950-7051(93)90017-N},
	language = {en},
	number = {4},
	urldate = {2021-08-25},
	journal = {Knowledge-Based Systems},
	author = {Dahlb{\"a}ck, N. and J{\"o}nsson, A. and Ahrenberg, L.},
	month = dec,
	year = {1993},
	pages = {258--266},
}

@article{sokol_one_2020,
	title = {One {Explanation} {Does} {Not} {Fit} {All}: {The} {Promise} of {Interactive} {Explanations} for {Machine} {Learning} {Transparency}},
	volume = {34},
	issn = {0933-1875, 1610-1987},
	shorttitle = {One {Explanation} {Does} {Not} {Fit} {All}},
	url = {http://link.springer.com/10.1007/s13218-020-00637-y},
	doi = {10.1007/s13218-020-00637-y},
	abstract = {Abstract The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system{\textquoteright}s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations{\textemdash}a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up {\textquotedblleft}What if?{\textquotedblright} questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee{\textquoteright}s mental model, which is the main building block of intelligible human{\textendash}machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend {\textquotedblleft}Wizard of Oz{\textquotedblright} studies as a proxy for testing and evaluating standalone interactive explainability algorithms.},
	language = {en},
	number = {2},
	urldate = {2021-08-25},
	journal = {KI - K{\"u}nstliche Intelligenz},
	author = {Sokol, Kacper and Flach, Peter},
	month = jun,
	year = {2020},
	pages = {235--250},
	file = {Volltext:/Users/ukuhl/Zotero/storage/68ISH96Q/Sokol und Flach - 2020 - One Explanation Does Not Fit All The Promise of I.pdf:application/pdf},
}

@article{detry_analyzing_2016,
	title = {Analyzing {Repeated} {Measurements} {Using} {Mixed} {Models}},
	volume = {315},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2015.19394},
	doi = {10.1001/jama.2015.19394},
	language = {en},
	number = {4},
	urldate = {2021-10-14},
	journal = {JAMA},
	author = {Detry, Michelle A. and Ma, Yan},
	month = jan,
	year = {2016},
	pages = {407},
	file = {Detry und Ma - 2016 - Analyzing Repeated Measurements Using Mixed Models.pdf:/Users/ukuhl/Zotero/storage/GEMAWZUT/Detry und Ma - 2016 - Analyzing Repeated Measurements Using Mixed Models.pdf:application/pdf},
}

@book{r_core_team_r_2021,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	year = {2021},
}

@article{bates_fitting_2015,
	title = {Fitting {Linear} {Mixed}-{Effects} {Models} {Using} \textbf{lme4}},
	volume = {67},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v67/i01/},
	doi = {10.18637/jss.v067.i01},
	language = {en},
	number = {1},
	urldate = {2021-11-16},
	journal = {Journal of Statistical Software},
	author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
	year = {2015},
	file = {Volltext:/Users/ukuhl/Zotero/storage/JZH5UMYC/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf:application/pdf},
}

@article{ben-shachar_effectsize_2020,
	title = {effectsize: {Estimation} of {Effect} {Size} {Indices} and {Standardized} {Parameters}},
	volume = {5},
	issn = {2475-9066},
	shorttitle = {effectsize},
	url = {https://joss.theoj.org/papers/10.21105/joss.02815},
	doi = {10.21105/joss.02815},
	number = {56},
	urldate = {2021-11-16},
	journal = {Journal of Open Source Software},
	author = {Ben-Shachar, Mattan and L{\"u}decke, Daniel and Makowski, Dominique},
	month = dec,
	year = {2020},
	pages = {2815},
	file = {Volltext:/Users/ukuhl/Zotero/storage/7QM6H6R7/Ben-Shachar et al. - 2020 - effectsize Estimation of Effect Size Indices and .pdf:application/pdf},
}

@article{bansal_updates_2019,
	title = {Updates in {Human}-{AI} {Teams}: {Understanding} and {Addressing} the {Performance}/{Compatibility} {Tradeoff}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Updates in {Human}-{AI} {Teams}},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/4087},
	doi = {10.1609/aaai.v33i01.33012429},
	abstract = {AI systems are being deployed to support human decision making in high-stakes domains such as healthcare and criminal justice. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI{\textquoteright}s inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of updates to an AI system in this setting. While updates can increase the AI{\textquoteright}s predictive performance, they may also lead to behavioral changes that are at odds with the user{\textquoteright}s prior experiences and confidence in the AI{\textquoteright}s inferences. We show that updates that increase AI performance may actually hurt team performance. We introduce the notion of the compatibility of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes classification tasks show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance/compatibility tradeoff across different datasets, enabling more compatible yet accurate updates.},
	urldate = {2021-11-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Bansal, Gagan and Nushi, Besmira and Kamar, Ece and Weld, Daniel S. and Lasecki, Walter S. and Horvitz, Eric},
	month = jul,
	year = {2019},
	pages = {2429--2437},
	file = {Volltext:/Users/ukuhl/Zotero/storage/GDKIPZAA/Bansal et al. - 2019 - Updates in Human-AI Teams Understanding and Addre.pdf:application/pdf},
}

@article{muth_alternative_2016,
	title = {Alternative {Models} for {Small} {Samples} in {Psychological} {Research}: {Applying} {Linear} {Mixed} {Effects} {Models} and {Generalized} {Estimating} {Equations} to {Repeated} {Measures} {Data}},
	volume = {76},
	issn = {0013-1644, 1552-3888},
	shorttitle = {Alternative {Models} for {Small} {Samples} in {Psychological} {Research}},
	url = {http://journals.sagepub.com/doi/10.1177/0013164415580432},
	doi = {10.1177/0013164415580432},
	abstract = {Unavoidable sample size issues beset psychological research that involves scarce populations or costly laboratory procedures. When incorporating longitudinal designs these samples are further reduced by traditional modeling techniques, which perform listwise deletion for any instance of missing data. Moreover, these techniques are limited in their capacity to accommodate alternative correlation structures that are common in repeated measures studies. Researchers require sound quantitative methods to work with limited but valuable measures without degrading their data sets. This article provides a brief tutorial and exploration of two alternative longitudinal modeling techniques, linear mixed effects models and generalized estimating equations, as applied to a repeated measures study ( n = 12) of pairmate attachment and social stress in primates. Both techniques provide comparable results, but each model offers unique information that can be helpful when deciding the right analytic tool.},
	language = {en},
	number = {1},
	urldate = {2021-11-18},
	journal = {Educational and Psychological Measurement},
	author = {Muth, Chelsea and Bales, Karen L. and Hinde, Katie and Maninger, Nicole and Mendoza, Sally P. and Ferrer, Emilio},
	month = feb,
	year = {2016},
	pages = {64--87},
	file = {Volltext:/Users/ukuhl/Zotero/storage/6YL7S6G2/Muth et al. - 2016 - Alternative Models for Small Samples in Psychologi.pdf:application/pdf},
}

@article{artelt_evaluating_2021,
	title = {Evaluating {Robustness} of {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/2103.02354},
	abstract = {Transparency is a fundamental requirement for decision making systems when these should be deployed in the real world. It is usually achieved by providing explanations of the system's behavior. A prominent and intuitive type of explanations are counterfactual explanations. Counterfactual explanations explain a behavior to the user by proposing actions {\textendash} as changes to the input {\textendash} that would cause a different (specified) behavior of the system. However, such explanation methods can be unstable with respect to small changes to the input {\textendash} i.e. even a small change in the input can lead to huge or arbitrary changes in the output and of the explanation. This could be problematic for counterfactual explanations, as two similar individuals might get very different explanations. Even worse, if the recommended actions differ considerably in their complexity, one would consider such unstable (counterfactual) explanations as individually unfair. In this work, we formally and empirically study the robustness of counterfactual explanations in general, as well as under different models and different kinds of perturbations. Furthermore, we propose that plausible counterfactual explanations can be used instead of closest counterfactual explanations to improve the robustness and consequently the individual fairness of counterfactual explanations.},
	urldate = {2021-11-30},
	journal = {arXiv:2103.02354 [cs]},
	author = {Artelt, Andr{\'e} and Vaquet, Valerie and Velioglu, Riza and Hinder, Fabian and Brinkrolf, Johannes and Schilling, Malte and Hammer, Barbara},
	month = jul,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/52R2A4AR/Artelt et al. - 2021 - Evaluating Robustness of Counterfactual Explanatio.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/RRE86KCW/2103.html:text/html},
}

@article{logan_shapes_1992,
	title = {Shapes of {Reaction}-{Time} {Distributions} and {Shapes} of {Learning} {Curves}: {A} {Test} of the {Instance} {Theory} of {Automaticity}},
	volume = {18},
	language = {en},
	number = {5},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Logan, Gordon D},
	year = {1992},
	pages = {883--914},
	file = {Logan - Shapes of Reaction-Time Distributions and Shapes o.pdf:/Users/ukuhl/Zotero/storage/GI8AD66Y/Logan - Shapes of Reaction-Time Distributions and Shapes o.pdf:application/pdf},
}

@article{artelt_efficient_2022,
	title = {Efficient computation of counterfactual explanations and counterfactual metrics of prototype-based classifiers},
	volume = {470},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221011024},
	doi = {10.1016/j.neucom.2021.04.129},
	language = {en},
	urldate = {2021-11-30},
	journal = {Neurocomputing},
	author = {Artelt, Andr{\'e} and Hammer, Barbara},
	month = jan,
	year = {2022},
	pages = {304--317},
}

@article{kumle_estimating_2021,
	title = {Estimating power in (generalized) linear mixed models: {An} open introduction and tutorial in {R}},
	volume = {53},
	issn = {1554-3528},
	shorttitle = {Estimating power in (generalized) linear mixed models},
	url = {https://link.springer.com/10.3758/s13428-021-01546-0},
	doi = {10.3758/s13428-021-01546-0},
	abstract = {Abstract Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to analytic power solutions are simulation-based power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
	language = {en},
	number = {6},
	urldate = {2021-11-30},
	journal = {Behavior Research Methods},
	author = {Kumle, Leah and V{\~o}, Melissa L.-H. and Draschkow, Dejan},
	month = may,
	year = {2021},
	pages = {2528--2543},
	file = {Volltext:/Users/ukuhl/Zotero/storage/I7A576AN/Kumle et al. - 2021 - Estimating power in (generalized) linear mixed mod.pdf:application/pdf},
}

@article{miller_explanation_2019,
	title = {Explanation in artificial intelligence: {Insights} from the social sciences},
	volume = {267},
	issn = {00043702},
	shorttitle = {Explanation in artificial intelligence},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	language = {en},
	urldate = {2021-11-30},
	journal = {Artificial Intelligence},
	author = {Miller, Tim},
	month = feb,
	year = {2019},
	pages = {1--38},
	file = {Eingereichte Version:/Users/ukuhl/Zotero/storage/8H3HEHQ5/Miller - 2019 - Explanation in artificial intelligence Insights f.pdf:application/pdf},
}

@article{gleaves_role_2020,
	title = {The {Role} of {Individual} {User} {Differences} in {Interpretable} and {Explainable} {Machine} {Learning} {Systems}},
	url = {http://arxiv.org/abs/2009.06675},
	abstract = {There is increased interest in assisting non-expert audiences to effectively interact with machine learning (ML) tools and understand the complex output such systems produce. Here, we describe user experiments designed to study how individual skills and personality traits predict interpretability, explainability, and knowledge discovery from ML generated model output. Our work relies on Fuzzy Trace Theory, a leading theory of how humans process numerical stimuli, to examine how different end users will interpret the output they receive while interacting with the ML system. While our sample was small, we found that interpretability {\textendash} being able to make sense of system output {\textendash} and explainability {\textendash} understanding how that output was generated {\textendash} were distinct aspects of user experience. Additionally, subjects were more able to interpret model output if they possessed individual traits that promote metacognitive monitoring and editing, associated with more detailed, verbatim, processing of ML output. Finally, subjects who are more familiar with ML systems felt better supported by them and more able to discover new patterns in data; however, this did not necessarily translate to meaningful insights. Our work motivates the design of systems that explicitly take users' mental representations into account during the design process to more effectively support end user requirements.},
	urldate = {2021-11-30},
	journal = {arXiv:2009.06675 [cs]},
	author = {Gleaves, Lydia P. and Schwartz, Reva and Broniatowski, David A.},
	month = sep,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/9KAPHRW2/Gleaves et al. - 2020 - The Role of Individual User Differences in Interpr.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/U4W37DQU/2009.html:text/html},
}

@article{smyth_few_2021,
	title = {A {Few} {Good} {Counterfactuals}: {Generating} {Interpretable}, {Plausible} and {Diverse} {Counterfactual} {Explanations}},
	shorttitle = {A {Few} {Good} {Counterfactuals}},
	url = {http://arxiv.org/abs/2101.09056},
	abstract = {Counterfactual explanations provide a potentially significant solution to the Explainable AI (XAI) problem, but good, native counterfactuals have been shown to rarely occur in most datasets. Hence, the most popular methods generate synthetic counterfactuals using blind perturbation. However, such methods have several shortcomings: the resulting counterfactuals (i) may not be valid data-points (they often use features that do not naturally occur), (ii) may lack the sparsity of good counterfactuals (if they modify too many features), and (iii) may lack diversity (if the generated counterfactuals are minimal variants of one another). We describe a method designed to overcome these problems, one that adapts native counterfactuals in the original dataset, to generate sparse, diverse synthetic counterfactuals from naturally occurring features. A series of experiments are reported that systematically explore parametric variations of this novel method on common datasets to establish the conditions for optimal performance.},
	urldate = {2021-11-30},
	journal = {arXiv:2101.09056 [cs]},
	author = {Smyth, Barry and Keane, Mark T.},
	month = jan,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/2T5ZXEHM/Smyth und Keane - 2021 - A Few Good Counterfactuals Generating Interpretab.pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/BF79LAYJ/2101.html:text/html},
}

@article{schleich_geco_2021,
	title = {{GeCo}: {Quality} {Counterfactual} {Explanations} in {Real} {Time}},
	shorttitle = {{GeCo}},
	url = {http://arxiv.org/abs/2101.01292},
	abstract = {Machine learning is increasingly applied in high-stakes decision making that directly affect people's lives, and this leads to an increased demand for systems to explain their decisions. Explanations often take the form of counterfactuals, which consists of conveying to the end user what she/he needs to change in order to improve the outcome. Computing counterfactual explanations is challenging, because of the inherent tension between a rich semantics of the domain, and the need for real time response. In this paper we present GeCo, the first system that can compute plausible and feasible counterfactual explanations in real time. At its core, GeCo relies on a genetic algorithm, which is customized to favor searching counterfactual explanations with the smallest number of changes. To achieve real-time performance, we introduce two novel optimizations: \${\textbackslash}textbackslashDelta\$-representation of candidate counterfactuals, and partial evaluation of the classifier. We compare empirically GeCo against five other systems described in the literature, and show that it is the only system that can achieve both high quality explanations and real time answers.},
	urldate = {2021-11-30},
	journal = {arXiv:2101.01292 [cs]},
	author = {Schleich, Maximilian and Geng, Zixuan and Zhang, Yihong and Suciu, Dan},
	month = may,
	year = {2021},
	keywords = {Computer Science - Databases, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ukuhl/Zotero/storage/44E4TKRS/Schleich et al. - 2021 - GeCo Quality Counterfactual Explanations in Real .pdf:application/pdf;arXiv.org Snapshot:/Users/ukuhl/Zotero/storage/J9SAN7F5/2101.html:text/html},
}

@article{johnson-laird_conditionals_2002,
	title = {Conditionals: {A} theory of meaning, pragmatics, and inference.},
	volume = {109},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Conditionals},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.109.4.646},
	doi = {10.1037/0033-295X.109.4.646},
	language = {en},
	number = {4},
	urldate = {2021-12-01},
	journal = {Psychological Review},
	author = {Johnson-Laird, P. N. and Byrne, Ruth M. J.},
	year = {2002},
	pages = {646--678},
	file = {Volltext:/Users/ukuhl/Zotero/storage/B63B39CT/Johnson-Laird und Byrne - 2002 - Conditionals A theory of meaning, pragmatics, and.pdf:application/pdf},
}

@article{byrne_mental_2002,
	title = {Mental models and counterfactual thoughts about what might have been},
	volume = {6},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661302019745},
	doi = {10.1016/S1364-6613(02)01974-5},
	language = {en},
	number = {10},
	urldate = {2021-12-01},
	journal = {Trends in Cognitive Sciences},
	author = {Byrne, Ruth M.J.},
	month = oct,
	year = {2002},
	pages = {426--431},
	file = {Byrne - 2002 - Mental models and counterfactual thoughts about wh.pdf:/Users/ukuhl/Zotero/storage/8H3S89H7/Byrne - 2002 - Mental models and counterfactual thoughts about wh.pdf:application/pdf},
}

@article{mcmullen_affective_2002,
	title = {Affective {Impact} of {Close} {Counterfactuals}: {Implications} of {Possible} {Futures} for {Possible} {Pasts}},
	volume = {38},
	issn = {00221031},
	shorttitle = {Affective {Impact} of {Close} {Counterfactuals}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022103101914829},
	doi = {10.1006/jesp.2001.1482},
	language = {en},
	number = {1},
	urldate = {2021-12-01},
	journal = {Journal of Experimental Social Psychology},
	author = {McMullen, Matthew N. and Markman, Keith D.},
	month = jan,
	year = {2002},
	pages = {64--70},
}

@article{byrne_precis_2007,
	title = {Pr{\'e}cis of \textit{{The}} {Rational} {Imagination}: {How} {People} {Create} {Alternatives} to {Reality}},
	volume = {30},
	issn = {0140-525X, 1469-1825},
	shorttitle = {Pr{\'e}cis of {\textbackslash}textlessi{\textbackslash}{textgreaterThe} {Rational} {Imagination}},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X07002579/type/journal_article},
	doi = {10.1017/S0140525X07002579},
	abstract = {Abstract The human imagination remains one of the last uncharted terrains of the mind. People often imagine how events might have turned out {\textquotedblleft}if only{\textquotedblright} something had been different. The {\textquotedblleft}fault lines{\textquotedblright} of reality, those aspects more readily changed, indicate that counterfactual thoughts are guided by the same principles as rational thoughts. In the past, rationality and imagination have been viewed as opposites. But research has shown that rational thought is more imaginative than cognitive scientists had supposed. In The Rational Imagination, I argue that imaginative thought is more rational than scientists have imagined. People exhibit remarkable similarities in the sorts of things they change in their mental representation of reality when they imagine how the facts could have turned out differently. For example, they tend to imagine alternatives to actions rather than inactions, events within their control rather than those beyond their control, and socially unacceptable events rather than acceptable ones. Their thoughts about how an event might have turned out differently lead them to judge that a strong causal relation exists between an antecedent event and the outcome, and their thoughts about how an event might have turned out the same lead them to judge that a weaker causal relation exists. In a simple temporal sequence, people tend to imagine alternatives to the most recent event. The central claim in the book is that counterfactual thoughts are organised along the same principles as rational thought. The idea that the counterfactual imagination is rational depends on three steps: (1) humans are capable of rational thought; (2) they make inferences by thinking about possibilities; and (3) their counterfactual thoughts rely on thinking about possibilities, just as rational thoughts do. The sorts of possibilities that people envisage explain the mutability of certain aspects of mental representations and the immutability of other aspects.},
	language = {en},
	number = {5-6},
	urldate = {2021-12-01},
	journal = {Behavioral and Brain Sciences},
	author = {Byrne, Ruth M. J.},
	month = dec,
	year = {2007},
	pages = {439--453},
}

@incollection{walsh_mental_2005,
	address = {London},
	series = {Routledge {Research} {International} {Series} in {Social} {Psychology}},
	title = {The mental representation of what might have been},
	booktitle = {The psychology of counterfactual thinking},
	publisher = {Routledge},
	author = {Walsh, C.R. and Byrne, Ruth M.J.},
	editor = {Mandel, D.R. and Hilton, Denis J. and Catellani, P.},
	year = {2005},
	pages = {61--73},
}

@incollection{lewis_counterfactuals_1973,
	address = {Dordrecht},
	title = {Counterfactuals and {Comparative} {Possibility}},
	isbn = {978-90-277-1220-2 978-94-009-9117-0},
	url = {http://link.springer.com/10.1007/978-94-009-9117-0_3},
	urldate = {2021-12-01},
	booktitle = {{IFS}},
	publisher = {Springer Netherlands},
	author = {Lewis, David},
	editor = {Harper, William L. and Stalnaker, Robert and Pearce, Glenn},
	year = {1973},
	doi = {10.1007/978-94-009-9117-0_3},
	pages = {57--85},
	file = {Lewis - 1973 - Counterfactuals and Comparative Possibility.pdf:/Users/ukuhl/Zotero/storage/M3VPHK8U/Lewis - 1973 - Counterfactuals and Comparative Possibility.pdf:application/pdf},
}

@article{stanley_counterfactual_2017,
	title = {Counterfactual {Plausibility} and {Comparative} {Similarity}},
	volume = {41},
	issn = {03640213},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.12451},
	doi = {10.1111/cogs.12451},
	language = {en},
	urldate = {2021-12-01},
	journal = {Cognitive Science},
	author = {Stanley, Matthew L. and Stewart, Gregory W. and Brigard, Felipe De},
	month = may,
	year = {2017},
	pages = {1216--1228},
	file = {Volltext:/Users/ukuhl/Zotero/storage/TJBSXIPX/Stanley et al. - 2017 - Counterfactual Plausibility and Comparative Simila.pdf:application/pdf},
}

@article{kulakova_processing_2013,
	title = {Processing counterfactual and hypothetical conditionals: {An} {fMRI} investigation},
	volume = {72},
	issn = {10538119},
	shorttitle = {Processing counterfactual and hypothetical conditionals},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811913001079},
	doi = {10.1016/j.neuroimage.2013.01.060},
	language = {en},
	urldate = {2021-12-01},
	journal = {NeuroImage},
	author = {Kulakova, Eugenia and Aichhorn, Markus and Schurz, Matthias and Kronbichler, Martin and Perner, Josef},
	month = may,
	year = {2013},
	pages = {265--271},
	file = {Volltext:/Users/ukuhl/Zotero/storage/WBTSJWR2/Kulakova et al. - 2013 - Processing counterfactual and hypothetical conditi.pdf:application/pdf},
}

@article{de_brigard_perceived_2021,
	title = {Perceived similarity of imagined possible worlds affects judgments of counterfactual plausibility},
	volume = {209},
	issn = {00100277},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027720303930},
	doi = {10.1016/j.cognition.2020.104574},
	language = {en},
	urldate = {2021-12-01},
	journal = {Cognition},
	author = {De Brigard, Felipe and Henne, Paul and Stanley, Matthew L.},
	month = apr,
	year = {2021},
	pages = {104574},
}

@article{connell_model_2006,
	title = {A {Model} of {Plausibility}},
	volume = {30},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1207/s15516709cog0000_53},
	doi = {10.1207/s15516709cog0000_53},
	language = {en},
	number = {1},
	urldate = {2021-12-01},
	journal = {Cognitive Science},
	author = {Connell, Louise and Keane, Mark T.},
	month = jan,
	year = {2006},
	pages = {95--120},
	file = {Connell und Keane - 2006 - A Model of Plausibility.pdf:/Users/ukuhl/Zotero/storage/NUB9C5BE/Connell und Keane - 2006 - A Model of Plausibility.pdf:application/pdf},
}

@article{byrne_temporality_2000,
	title = {The temporality effect in counterfactual thinking about what might have been},
	volume = {28},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/BF03213805},
	doi = {10.3758/BF03213805},
	language = {en},
	number = {2},
	urldate = {2021-12-02},
	journal = {Memory \& Cognition},
	author = {Byrne, Ruth M. J. and Segura, Susana and Culhane, Ronan and Tasso, Alessandra and Berrocal, Pablo},
	month = mar,
	year = {2000},
	pages = {264--281},
	file = {Volltext:/Users/ukuhl/Zotero/storage/2BVSZ82R/Byrne et al. - 2000 - The temporality effect in counterfactual thinking .pdf:application/pdf},
}

@article{miller_temporal_1990,
	title = {Temporal order and the perceived mutability of events: {Implications} for blame assignment.},
	volume = {59},
	issn = {1939-1315, 0022-3514},
	shorttitle = {Temporal order and the perceived mutability of events},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.59.6.1111},
	doi = {10.1037/0022-3514.59.6.1111},
	language = {en},
	number = {6},
	urldate = {2021-12-02},
	journal = {Journal of Personality and Social Psychology},
	author = {Miller, Dale T. and Gunasegaram, Saku},
	year = {1990},
	pages = {1111--1118},
}

@article{dixon_if_2011,
	title = {{\textquotedblleft}{If} only{\textquotedblright} counterfactual thoughts about exceptional actions},
	volume = {39},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/s13421-011-0101-4},
	doi = {10.3758/s13421-011-0101-4},
	language = {en},
	number = {7},
	urldate = {2021-12-02},
	journal = {Memory \& Cognition},
	author = {Dixon, James E. and Byrne, Ruth M. J.},
	month = oct,
	year = {2011},
	pages = {1317--1331},
}

@article{girotto_event_1991,
	title = {Event controllability in counterfactual thinking},
	volume = {78},
	issn = {00016918},
	url = {https://linkinghub.elsevier.com/retrieve/pii/000169189190007M},
	doi = {10.1016/0001-6918(91)90007-M},
	language = {en},
	number = {1-3},
	urldate = {2021-12-02},
	journal = {Acta Psychologica},
	author = {Girotto, Vittorio and Legrenzi, Paolo and Rizzo, Antonio},
	month = dec,
	year = {1991},
	pages = {111--133},
}

@article{pezdek_is_2006,
	title = {Is knowing believing? {The} role of event plausibility and background knowledge in planting false beliefs about the personal past},
	volume = {34},
	issn = {0090-502X, 1532-5946},
	shorttitle = {Is knowing believing?},
	url = {http://link.springer.com/10.3758/BF03195925},
	doi = {10.3758/BF03195925},
	language = {en},
	number = {8},
	urldate = {2021-12-02},
	journal = {Memory \& Cognition},
	author = {Pezdek, Kathy and Blandon-Gitlin, Iris and Lam, Shirley and Hart, Rhiannon Ellis and Schooler, Jonathan W.},
	month = dec,
	year = {2006},
	pages = {1628--1635},
}

@article{de_brigard_remembering_2013,
	title = {Remembering what could have happened: {Neural} correlates of episodic counterfactual thinking},
	volume = {51},
	issn = {00283932},
	shorttitle = {Remembering what could have happened},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0028393213000298},
	doi = {10.1016/j.neuropsychologia.2013.01.015},
	language = {en},
	number = {12},
	urldate = {2021-12-02},
	journal = {Neuropsychologia},
	author = {De Brigard, F. and Addis, D.R. and Ford, J.H. and Schacter, D.L. and Giovanello, K.S.},
	month = oct,
	year = {2013},
	pages = {2401--2414},
	file = {Akzeptierte Version:/Users/ukuhl/Zotero/storage/PRZSCFDZ/De Brigard et al. - 2013 - Remembering what could have happened Neural corre.pdf:application/pdf},
}

@article{medvec_when_1997,
	title = {When doing better means feeling worse: {The} effects of categorical cutoff points on counterfactual thinking and satisfaction.},
	volume = {72},
	issn = {1939-1315, 0022-3514},
	shorttitle = {When doing better means feeling worse},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.72.6.1284},
	doi = {10.1037/0022-3514.72.6.1284},
	language = {en},
	number = {6},
	urldate = {2021-12-03},
	journal = {Journal of Personality and Social Psychology},
	author = {Medvec, Victoria Husted and Savitsky, Kenneth},
	year = {1997},
	pages = {1284--1296},
}

@inproceedings{wang_designing_2019,
	address = {Glasgow Scotland Uk},
	title = {Designing {Theory}-{Driven} {User}-{Centric} {Explainable} {AI}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300831},
	doi = {10.1145/3290605.3300831},
	language = {en},
	urldate = {2021-12-06},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y.},
	month = may,
	year = {2019},
	pages = {1--15},
}
